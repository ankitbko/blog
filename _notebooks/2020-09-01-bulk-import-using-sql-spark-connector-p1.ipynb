{"cells":[{"cell_type":"markdown","source":["# Building large scale data ingestion solutions for Azure SQL using Azure databricks - Part 1\n> Discover how to bulk insert million of rows into Azure SQL Hyperscale using Databricks\n\n- toc: false\n- badges: false\n- comments: true\n- categories: [spark, Azure Databricks, Azure SQL, data ingestion, SQL spark connector, big data, python]\n- hide: false\n- search_exclude: false\n- source_code: https://github.com/ankitbko/sql-spark-connector-sample\n- image: images/previews/spark-connector-1-preview.svg"],"metadata":{}},{"cell_type":"markdown","source":["With rise of big data, polyglot persistence and availability of cheaper storage technology it is becoming increasingly common to keep data into cheaper long term storage such as ADLS and load them into OLTP or OLAP databases as needed. In this 3 part blog series we will check out newly released [Apache Spark Connector for SQL Server and Azure SQL](https://github.com/microsoft/sql-spark-connector) (refered as Microsoft SQL Spark Connector) and use it to insert large amount of data into Azure SQL Hyperscale. We will also capture benchmarks and also discuss some common problems we faced and solutions to them.  \n","\n","In this first post we will see to use Microsoft SQL Spark Connector to bulk load data into Azure SQL from Azure Data Lake and how to optimize it even further. In the second part we will capture and compare benchmarks of bulk loading large dataset into different Azure SQL databases each having different indexing strategy. And in the final post we will discuss an issue with deadlocks (that we will face along this journey) and potential solultion. In each of the posts in this series I will mention what environment and dataset I have used and also share direct links to the scripts used.\n","\n","So lets get started."],"metadata":{}},{"cell_type":"markdown","source":["## Getting started with Microsoft SQL Spark Connector\n\nMicrosoft SQL Spark Connector is an evolution of now deprecated [Azure SQL Spark Connector](https://github.com/Azure/azure-sqldb-spark). It provides hosts of different features to easily integrate with SQL Server and Azure SQL from spark. At the time of writing this blog, the connector is in active development and a release package is not yet published to maven repository. So in order to get it you can either download precompiled *jar* file from the [releases](https://github.com/microsoft/sql-spark-connector/releases) tab in repository, or build the master branch locally. Once you have the jar file install it into your databricks cluster.  \n\n> Note: As of writing this post, the connector does not support Spark 3.0."],"metadata":{}},{"cell_type":"markdown","source":["## Environment\n\nWe will be using Azure Databricks with cluster configurations as following - \n\n- Cluster Mode: Standard\n- Databricks Runtime Version: 6.6 (includes Apache Spark 2.4.5, Scala 2.11)\n- Workers: 2 \n- Worker Type: Standard_DS3_v2 (14.0 GB Memory, 4 Cores, 0.75 DBU)\n- Driver Type: Standard_DS3_v2 (14.0 GB Memory, 4 Cores, 0.75 DBU)\n\nLibraries installed in the cluster - \n- Microsoft SQL Spark Connector (jar) - Built at [c787e8f](https://github.com/microsoft/sql-spark-connector/tree/c787e8ff854eb356c31092c9815e3a43617d9b4f)\n- codetiming (PyPI) - Used to capture metrics\n- altair - Used to display charts\n\nAzure SQL Server Hyperscale configured at 2vCore and 0 replicas. In this post we will be using a single database which has tables as per this [SQL DDL script](https://github.com/ankitbko/sql-spark-connector-sample/blob/master/sql/create_table.sql).\n\nAzure Data Lake Gen 2 contains parquet files for the dataset we use which is then [mounted](https://docs.databricks.com/data/data-sources/azure/azure-datalake-gen2.html#mount-an-azure-data-lake-storage-gen2-account-using-a-service-principal-and-oauth-20) on Databricks."],"metadata":{}},{"cell_type":"markdown","source":["## Dataset\n\nWe will be using 1 TB TPC-DS dataset v2.13.0rc1 for this blog series. Due to licensing restriction TPC-DS tool is not included the repo. However the toolkit is free to download [here](http://www.tpc.org/tpcds/default5.asp). However I did include a subset of SQL DDL statements for creating tables [here](https://github.com/ankitbko/sql-spark-connector-sample/blob/master/sql/create_table.sql) that we use in this blog. \n\nIn this post we will be focusing on only 3 tables - store, store_returns and store_sales. As the name suggests *store_sales* and *store_returns* contains items sold or returned for different stores. For more information about the dataset refer to the specification document available in the TPC-DS toolkit.\n\nThere are no foreign key in the tables so Surrogate Key of store (`s_store_sk`) will be used to query and group results from store_sales (`ss_store_sk`) and store_returns (`sr_store_sk`).\n\nThis dataset was already available in my Azure SQL database and is then exported as parquet files using [export notebook](https://github.com/ankitbko/sql-spark-connector-sample/blob/master/notebooks/00-export-parquet-sql.ipynb). In case you have the dataset as text files you can modify the export notebook accordingly. \n\n> Note: If you generate parquet files from any text format such as (csv, pipe-separated etc) you will need to correctly specify schema while writing to parquet. You can find the schema in specification document of TPC-DS toolkit."],"metadata":{"tags":[]}},{"cell_type":"code","source":["#hide_input\n[f.path for f in dbutils.fs.ls(\"/mnt/adls/adls7dataset7benchmark/tpcds1tb/parquet/store_sales\")][:5]"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[1]: [&#39;dbfs:/mnt/adls/adls7dataset7benchmark/tpcds1tb/parquet/store_sales/_SUCCESS&#39;,\n &#39;dbfs:/mnt/adls/adls7dataset7benchmark/tpcds1tb/parquet/store_sales/ss_store_sk=1/&#39;,\n &#39;dbfs:/mnt/adls/adls7dataset7benchmark/tpcds1tb/parquet/store_sales/ss_store_sk=10/&#39;,\n &#39;dbfs:/mnt/adls/adls7dataset7benchmark/tpcds1tb/parquet/store_sales/ss_store_sk=100/&#39;,\n &#39;dbfs:/mnt/adls/adls7dataset7benchmark/tpcds1tb/parquet/store_sales/ss_store_sk=1000/&#39;]</div>"]}}],"execution_count":6},{"cell_type":"markdown","source":["Notice in the above output the *store_sales* dataset is partitioned on `ss_store_sk`. This partitioning strategy works for us as we will be filtering on `ss_store_sk` and only work with few of the stores. This will also set us up for an issue that we will face later. The same partitioning strategy is used for *store_returns* as well."],"metadata":{}},{"cell_type":"markdown","source":["## Bulk Loading data into Azure SQL Database\n\nOur use case will be to load sales and returns for a particular *store* into Azure SQL database having row store indexes (Primary Key) on table. This means we will have to load data for each store from *store* table and all its associated sales and returns from *store_sales* and *store_returns* tables respectively. Surrogate key for store is what ties together all three tables.\n\nTo compare results we will capture the time it takes to insert records into each table. Therefore there are some boilerplate code that captures metrics which can be safely removed. Anything within `# --- Capturing Metrics Start---` and `# --- Capturing Metrics End---` block is used solely for capturing metrics and can be safely ignored. It does not have any impact on importing data into Azure SQL."],"metadata":{}},{"cell_type":"code","source":["from codetiming import Timer\nimport pandas as pd\nimport os\n\npath = \"/mnt/adls/adls7dataset7benchmark/tpcds1tb/parquet/\"\nserver_name = dbutils.secrets.get(scope = \"kvbenchmark\", key = \"db-server-url\")\nusername = dbutils.secrets.get(scope = \"kvbenchmark\", key = \"db-username\")\npassword = dbutils.secrets.get(scope = \"kvbenchmark\", key = \"db-password\")\nschema = \"dbo\"\n\ndatabase_name = \"noidx\"\nurl = server_name + \";\" + \"databaseName=\" + database_name + \";\"\nurl"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[2]: &#39;[REDACTED];databaseName=noidx;&#39;</div>"]}}],"execution_count":9},{"cell_type":"markdown","source":["We start with importing relevant packages and getting secrets from our secret-store. In case you want to run the notebook for yourself, you will need to modify the above cell accordingly. I am using `codetiming` and `pandas` packages to capture metrics and is not needed for bulk importing itself."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.types import *\n\n# List of table names\ntables = [\"store\", \"store_returns\", \"store_sales\"]\n\n# Map between table names and store surrogate key\ntable_storesk_map = {\n  \"store\": \"s_store_sk\", \n  \"store_returns\": \"sr_store_sk\",\n  \"store_sales\": \"ss_store_sk\"\n}\n\n# Map between table names and schema\ntable_schema_map = {\n  \"store\": [\n    StructField(\"s_store_sk\", IntegerType(), False),\n    StructField(\"s_store_id\", StringType(), False)\n  ],\n  \"store_returns\": [\n    StructField(\"sr_item_sk\", IntegerType(), False), \n    StructField(\"sr_ticket_number\", IntegerType(), False),\n  ],\n  \"store_sales\": [\n    StructField(\"ss_item_sk\", IntegerType(), False), \n    StructField(\"ss_ticket_number\", IntegerType(), False)\n  ]\n}\n\n# List of stores that we use\nstores = [row[\"ss_store_sk\"] for row in spark.read.parquet(f\"{path}/store_sales\").filter(\"ss_store_sk IS NOT null\").groupBy('ss_store_sk').count().orderBy('count', ascending=False).select(\"ss_store_sk\").take(5)]\nprint(stores)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">[529, 650, 14, 406, 178]\n</div>"]}}],"execution_count":11},{"cell_type":"markdown","source":["Next we initialize the tables and *stores* that we will use for importing into Azure SQL. `table_schema_map` is used to correct the schema to match that of database. This is a temporary workaround for [issue #5](https://github.com/microsoft/sql-spark-connector/issues/5). We will be bulk inserting data in *store*, *store_sales* and *store_returns* for 5 stores having highest number of records in *store_sales*. The count of records for each of the stores are as below. Overall we will be inserting ~30 million records into our database."],"metadata":{}},{"cell_type":"code","source":["#hide_input\ncount_records = {store_sk: {table_name: spark.read.parquet(f\"{path}/{table_name}\").where(f\"{table_storesk_map[table_name]} == {store_sk}\").count() for table_name in tables} for store_sk in stores}\n\nc_df = pd.DataFrame(count_records).transpose()\nc_df[\"Total\"] = c_df.sum(numeric_only=True, axis=1)\nc_df"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>store</th>\n      <th>store_returns</th>\n      <th>store_sales</th>\n      <th>Total</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>529</th>\n      <td>1</td>\n      <td>555343</td>\n      <td>5512441</td>\n      <td>6067785</td>\n    </tr>\n    <tr>\n      <th>650</th>\n      <td>1</td>\n      <td>555966</td>\n      <td>5510505</td>\n      <td>6066472</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>1</td>\n      <td>554904</td>\n      <td>5508259</td>\n      <td>6063164</td>\n    </tr>\n    <tr>\n      <th>406</th>\n      <td>1</td>\n      <td>554828</td>\n      <td>5506912</td>\n      <td>6061741</td>\n    </tr>\n    <tr>\n      <th>178</th>\n      <td>1</td>\n      <td>554147</td>\n      <td>5506321</td>\n      <td>6060469</td>\n    </tr>\n  </tbody>\n</table>\n</div>"]}}],"execution_count":13},{"cell_type":"markdown","source":["I have a method to truncate the tables that we are working with. This will make it easier for us to rerun any of tests without getting into trouble with duplicate Primary key."],"metadata":{}},{"cell_type":"code","source":["def truncate_tables(url, username, password, tables):\n  for table in tables:\n    query = f\"TRUNCATE TABLE {schema}.{table}\"\n\n    try:\n      t = Timer(text=f\"Tuncated table {table} in: {{:0.2f}}\")\n      t.start()\n      driver_manager = spark._sc._gateway.jvm.java.sql.DriverManager\n      con = driver_manager.getConnection(url, username, password)\n\n      stmt = con.createStatement()\n      stmt.executeUpdate(query)\n      stmt.close()\n      t.stop()\n    except Exception as e:\n      print(f\"Failed to truncate table {table}\", e)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":15},{"cell_type":"markdown","source":["### Inserting into each table sequentially"],"metadata":{}},{"cell_type":"code","source":["#hide\n# Used to capture metrics. Can be ignored.\nmetrics_seq_df = pd.DataFrame(columns=['store_sk','store', 'store_returns', \"store_sales\", 'Total'])"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":17},{"cell_type":"code","source":["# Temporary workaround until Issue #5 gets fixed https://github.com/microsoft/sql-spark-connector/issues/5\ndef create_valid_table_schema(table_name, df_schema): \n  return StructType([x if x.name not in map(lambda n: n.name, table_schema_map[table_name]) else next(filter(lambda f: f.name == x.name ,table_schema_map[table_name])) for x in df_schema])\n\ndef import_single_store(url, table_name, store_sk): \n  try:\n      df = spark.read.parquet(f\"{path}/{table_name}\")\n      df = df.where(f\"{table_storesk_map[table_name]} == {store_sk}\")\n\n      # Temporary workaround until Issue #5 gets fixed https://github.com/microsoft/sql-spark-connector/issues/5\n      table_schema = create_valid_table_schema(table_name, df.schema)\n      df = spark.createDataFrame(df.rdd, table_schema)\n\n      t = Timer(text=f\"Imported store {store_sk} into table {table_name} in : {{:0.2f}} \")    \n      t.start()\n      \n      df.write.format(\"com.microsoft.sqlserver.jdbc.spark\") \\\n        .mode(\"append\") \\\n        .option(\"url\", url) \\\n        .option(\"dbtable\", f\"{schema}.{table_name}\") \\\n        .option(\"user\", username) \\\n        .option(\"password\", password) \\\n        .option(\"tableLock\", \"false\") \\\n        .option(\"batchsize\", \"100000\") \\\n        .save()\n\n      elapsed = t.stop()\n      return elapsed\n  except Exception as e:\n    print(f\"Failed to import {store_sk} into table {table_name}\", e)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":18},{"cell_type":"markdown","source":["These two methods will be reused throught this blog. `create_valid_table_schema` method takes the schema of the parquet files and merges it with schema defined in `table_schema_map`. This is a temporary fix as discussed before. \n\n`import_single_store(url, table_name, store_sk)` takes in the url of the database, the table name and the surrogate key (`store_sk`) of the store. It then reads the parquet file for the specified table filtered by `store_sk`. It corrects the schema, starts the timer and submits the insertion job to spark. The two parameters `tableLock` and `batchsize` are crucial. \n\n- `tableLock` or [TABLOCK](https://docs.microsoft.com/en-us/sql/t-sql/queries/hints-transact-sql-table?view=sql-server-ver15) specifies that the acquired lock is applied at the table level. This option is helpful when inserting into Heap table.\n- `batchsize` describes how many rows are committed at a time during the bulk operation. Play around with it to see how the system performs with different batch sizes. This option will become useful later when we bulk insert into a Clustered Columnstore Index (CCI)."],"metadata":{}},{"cell_type":"code","source":["def import_stores(url):\n  for store_sk in stores:      \n      # -------- Capturing Metrics Start---------\n      t = Timer(text=f\"Imported store {store_sk} in : {{:0.2f}}\")\n      t.start()\n      metrics_row = {'store_sk': f'{store_sk}'}\n      # -------- Capturing Metrics End---------\n      \n      for table_name in table_storesk_map.keys():\n        elapsed = import_single_store(url, table_name, store_sk)\n\n        # -------- Capturing Metrics Start---------\n        metrics_row[table_name] = elapsed\n      \n      elapsed = t.stop()\n      metrics_row['Total'] = elapsed\n      global metrics_seq_df\n      metrics_seq_df = metrics_seq_df.append(metrics_row, ignore_index=True)\n      # -------- Capturing Metrics End---------"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":20},{"cell_type":"markdown","source":["`import_stores` loops over all stores and calls `import_single_store` for each table. Note that it waits for `import_single_store` to complete before continuing the loop and inserting into next table. Lets truncate the tables and run it to capture the reults."],"metadata":{}},{"cell_type":"code","source":["truncate_tables(url, username, password, tables)\nimport_stores(url)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Tuncated table store in: 0.17\nTuncated table store_returns in: 0.06\nTuncated table store_sales in: 0.03\nImported store 529 into table store in : 2.72 \nImported store 529 into table store_returns in : 27.67 \nImported store 529 into table store_sales in : 276.37 \nImported store 529 in : 311.85\nImported store 650 into table store in : 0.59 \nImported store 650 into table store_returns in : 26.83 \nImported store 650 into table store_sales in : 151.32 \nImported store 650 in : 184.33\nImported store 14 into table store in : 0.31 \nImported store 14 into table store_returns in : 28.14 \nImported store 14 into table store_sales in : 182.92 \nImported store 14 in : 216.08\nImported store 406 into table store in : 0.43 \nImported store 406 into table store_returns in : 42.02 \nImported store 406 into table store_sales in : 310.86 \nImported store 406 in : 358.28\nImported store 178 into table store in : 0.31 \nImported store 178 into table store_returns in : 104.81 \nImported store 178 into table store_sales in : 581.17 \nImported store 178 in : 691.08\n</div>"]}}],"execution_count":22},{"cell_type":"markdown","source":["The import is going to succeed and the timings will be logged for each table.\n\nThere are two interesting things to notice when the jobs are running. First is that only one spark job runs at a time. This is because we are inserting into one table at a time and the spark will create only 1 job for each `spark.write` when inserting data into Azure SQL. This is important fact to keep in mind to optimize in future.\n\n![spark_job](./assets/images/posts/spark-connector-1/spark_jobs.png)\n\nThe second is a little tricky to find. If you check the stages of running job when it inserts into `store_sales` table in Spark UI you will notice some tasks will fail due to Deadlock. \n`com.microsoft.sqlserver.jdbc.SQLServerException: Transaction (Process ID 99) was deadlocked on lock resources with another process and has been chosen as the deadlock victim. Rerun the transaction.`\n\nAlthough these tasks failed the stage and job overall succeeded and this is due to retry mechanism within Databricks. This can be confirmed by counting number of records in database to perform a quick sanity check. We let it be for now as we will be discussing the root cause of deadlock and potential solutions in part 3 of this blog.\n\n![deadlock](./assets/images/posts/spark-connector-1/deadlock.png)"],"metadata":{}},{"cell_type":"code","source":["#hide_input\n\n# Capturing metrics. Can be ignored.\nmetrics_seq_df"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>store_sk</th>\n      <th>store</th>\n      <th>store_returns</th>\n      <th>store_sales</th>\n      <th>Total</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>529</td>\n      <td>2.717032</td>\n      <td>27.669652</td>\n      <td>276.367528</td>\n      <td>311.850131</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>650</td>\n      <td>0.592542</td>\n      <td>26.830166</td>\n      <td>151.320050</td>\n      <td>184.332525</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>14</td>\n      <td>0.307472</td>\n      <td>28.139986</td>\n      <td>182.920102</td>\n      <td>216.076222</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>406</td>\n      <td>0.427487</td>\n      <td>42.020107</td>\n      <td>310.856628</td>\n      <td>358.281521</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>178</td>\n      <td>0.307313</td>\n      <td>104.811686</td>\n      <td>581.166653</td>\n      <td>691.082328</td>\n    </tr>\n  </tbody>\n</table>\n</div>"]}}],"execution_count":24},{"cell_type":"markdown","source":["If you look at the timings it becomes obvious that overall time it took to insert all the data related to single store is sum of time it took to insert into each of the tables for that store. But can we do any better? Ususally in most system partially inserted data is not of use so we are delayed till the data is inserted for all related tables. So our next objective is to reduce the time it takes to insert all data for a single store. Wouldn't it be great if we can insert data for single store in all the tables concurrently?"],"metadata":{}},{"cell_type":"markdown","source":["### Concurrently inserting data in all tables\n\nTo achieve this we will utilize Python's multithreading library to submit multiple spark jobs concurrently. As you would know, anything we write and execute through databricks notebook runs on executor. And when executor find statement related to spark, it submits the job to spark and spark then orchestrates its execution on the workers. The observation from above tells us that only single job (and single stage) is executing in our cluster at any moment. This means our cluster is woefully underutilized. Our plan is to submit multiple spark jobs to better utilize our cluster.\n\nTo make it work properly with python we have to set `PYSPARK_PIN_THREAD` environment variable in python and set it to `true`. You need to do this as part of cluster startup as setting it though Python's `os` module does not work. You can find more details about it [in the doc](https://spark.apache.org/docs/2.4.5/job-scheduling.html#concurrent-jobs-in-pyspark). Note that as of yet this flag is not recommended in production. In case you want to perform such parallelization its better to use Scala. You can achieve same result in Scala using `Futures` and `Awaits`."],"metadata":{}},{"cell_type":"code","source":["#hide\n\n# For capturing metrics. \nmetrics_con_df = pd.DataFrame(columns=['store_sk','store', 'store_returns', \"store_sales\", 'Total'])"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":27},{"cell_type":"code","source":["import threading\n\nclass myThread (threading.Thread):\n   def __init__(self, table_name, store_sk):\n      threading.Thread.__init__(self)\n      self.table_name = table_name\n      self.store_sk = store_sk\n  \n   def run(self):      \n      elapsed = import_single_store(url, self.table_name, self.store_sk)\n\n      # -------- Capturing Metrics Start---------\n      global metrics_con_df\n      metrics_con_df.loc[metrics_con_df['store_sk'] == f'{self.store_sk}', self.table_name] = elapsed\n      # -------- Capturing Metrics End---------\n\n\ndef import_stores_concurrent():\n  for store_sk in stores:\n      \n      # -------- Capturing Metrics Start---------\n      time = Timer(text=f\"Imported store {store_sk} in : {{:0.2f}}\")\n      time.start()\n      \n      global metrics_con_df\n      metrics_con_df = metrics_con_df.append({'store_sk': f'{store_sk}'}, ignore_index=True)\n      # -------- Capturing Metrics End---------\n            \n      threads = []\n      \n      for table_name in tables:\n          thread = myThread(table_name, store_sk)\n          thread.start()\n          threads.append(thread)\n\n      # Wait for all threads to complete\n      for t in threads:\n          t.join()\n      \n      # -------- Capturing Metrics Start--------- \n      elapsed = time.stop()\n      metrics_con_df.loc[metrics_con_df['store_sk'] == f'{store_sk}', 'Total'] = elapsed\n      # -------- Capturing Metrics End---------"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":28},{"cell_type":"markdown","source":["We start with creating a class with most creative name ever - `myThread`. Each of myThread will execute `import_single_store` for a single table for a store. `import_stores_concurrent` loops through the stores and for each table it creates a new `myThread` that executes `import_single_store`. Each of this thread will submit a job to spark and spark will then handle the orchestration of the jobs on the cluster. After submitting all the jobs we wait for all the threads to finish before continuing with next store in loop. This means that for every store we will be inserting data from all three tables concurrently."],"metadata":{}},{"cell_type":"code","source":["truncate_tables(url, username, password, tables)\nimport_stores_concurrent()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Tuncated table store in: 0.08\nTuncated table store_returns in: 0.08\nTuncated table store_sales in: 0.06\nImported store 529 into table store in : 0.29 \nImported store 529 into table store_returns in : 26.18 \nImported store 529 into table store_sales in : 274.94 \nImported store 529 in : 279.10\nImported store 650 into table store in : 3.33 \nImported store 650 into table store_returns in : 42.55 \nImported store 650 into table store_sales in : 161.23 \nImported store 650 in : 165.20\nImported store 14 into table store in : 0.27 \nImported store 14 into table store_returns in : 41.86 \nImported store 14 into table store_sales in : 213.83 \nImported store 14 in : 217.59\nImported store 406 into table store in : 0.30 \nImported store 406 into table store_returns in : 108.57 \nImported store 406 into table store_sales in : 331.98 \nImported store 406 in : 335.90\nImported store 178 into table store in : 0.32 \nImported store 178 into table store_returns in : 135.44 \nImported store 178 into table store_sales in : 704.72 \nImported store 178 in : 708.38\n</div>"]}}],"execution_count":30},{"cell_type":"code","source":["#hide_input\nmetrics_con_df"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>store_sk</th>\n      <th>store</th>\n      <th>store_returns</th>\n      <th>store_sales</th>\n      <th>Total</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>529</td>\n      <td>0.288181</td>\n      <td>26.176</td>\n      <td>274.942</td>\n      <td>279.098</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>650</td>\n      <td>3.32614</td>\n      <td>42.5514</td>\n      <td>161.225</td>\n      <td>165.2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>14</td>\n      <td>0.272375</td>\n      <td>41.8554</td>\n      <td>213.833</td>\n      <td>217.59</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>406</td>\n      <td>0.304694</td>\n      <td>108.566</td>\n      <td>331.975</td>\n      <td>335.902</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>178</td>\n      <td>0.319758</td>\n      <td>135.438</td>\n      <td>704.716</td>\n      <td>708.383</td>\n    </tr>\n  </tbody>\n</table>\n</div>"]}}],"execution_count":31},{"cell_type":"markdown","source":["This time you will notice that multiple spark jobs are running at same time. This becomes more evident once we look at the captured timings. The total time take for a single store is not the sum of each table but rather MAX for all 3 tables which in our case is `store_sales`. Now there may be some additional cost involved while scheduling jobs therefore the timings does not match exactly with the `store_sales`. However point to note here is that the time taken to insert into `store_returns` is completely absorbed by the timing of `store_sales`."],"metadata":{}},{"cell_type":"code","source":["#hide\nmetrics_con_df2 = metrics_con_df\nmetrics_con_df2['Total (sequential)'] = metrics_con_df2[['store','store_returns','store_sales']].sum(axis=1)\nmetrics_con_df2\n\nmc_df = metrics_con_df[[\"store_sk\", \"Total\"]].melt('store_sk', var_name='table name', value_name='Time (s)')\nmc_df['type'] = \"concurrent\"\n\nms_df = metrics_con_df2[[\"store_sk\", \"Total (sequential)\"]].rename(columns = {'Total (sequential)': 'Total'}, inplace=False).melt('store_sk', var_name='table name', value_name='Time (s)')\nms_df['type'] = \"sequential\"\n\nm_df = mc_df.append(ms_df)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":33},{"cell_type":"code","source":["#hide_input\nimport altair as alt\n\nalt.Chart(m_df).mark_bar().encode(\n    x=alt.Column('type:N', sort=alt.SortField(\"type\", order=\"descending\")),\n    y='Time (s):Q',\n    color='type:N',\n    column='store_sk:N',\n    tooltip=[alt.Tooltip('store_sk:N'),\n             alt.Tooltip('type:N'),\n             alt.Tooltip('Time (s):Q')]\n)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["\n<div id=\"altair-viz-905eb7ee374f4b38975cc3b72af18c47\"></div>\n<script type=\"text/javascript\">\n  (function(spec, embedOpt){\n    let outputDiv = document.currentScript.previousElementSibling;\n    if (outputDiv.id !== \"altair-viz-905eb7ee374f4b38975cc3b72af18c47\") {\n      outputDiv = document.getElementById(\"altair-viz-905eb7ee374f4b38975cc3b72af18c47\");\n    }\n    const paths = {\n      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.8.1?noext\",\n      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n    };\n\n    function loadScript(lib) {\n      return new Promise(function(resolve, reject) {\n        var s = document.createElement('script');\n        s.src = paths[lib];\n        s.async = true;\n        s.onload = () => resolve(paths[lib]);\n        s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n        document.getElementsByTagName(\"head\")[0].appendChild(s);\n      });\n    }\n\n    function showError(err) {\n      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n      throw err;\n    }\n\n    function displayChart(vegaEmbed) {\n      vegaEmbed(outputDiv, spec, embedOpt)\n        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n    }\n\n    if(typeof define === \"function\" && define.amd) {\n      requirejs.config({paths});\n      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n    } else if (typeof vegaEmbed === \"function\") {\n      displayChart(vegaEmbed);\n    } else {\n      loadScript(\"vega\")\n        .then(() => loadScript(\"vega-lite\"))\n        .then(() => loadScript(\"vega-embed\"))\n        .catch(showError)\n        .then(() => displayChart(vegaEmbed));\n    }\n  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-fac6c0a2c69463a15a1ef8946bbbe960\"}, \"mark\": \"bar\", \"encoding\": {\"color\": {\"type\": \"nominal\", \"field\": \"type\"}, \"column\": {\"type\": \"nominal\", \"field\": \"store_sk\"}, \"tooltip\": [{\"type\": \"nominal\", \"field\": \"store_sk\"}, {\"type\": \"nominal\", \"field\": \"type\"}, {\"type\": \"quantitative\", \"field\": \"Time (s)\"}], \"x\": {\"type\": \"nominal\", \"field\": \"type\", \"sort\": {\"field\": \"type\", \"order\": \"descending\"}}, \"y\": {\"type\": \"quantitative\", \"field\": \"Time (s)\"}}, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.8.1.json\", \"datasets\": {\"data-fac6c0a2c69463a15a1ef8946bbbe960\": [{\"store_sk\": \"529\", \"table name\": \"Total\", \"Time (s)\": 279.09817834700016, \"type\": \"concurrent\"}, {\"store_sk\": \"650\", \"table name\": \"Total\", \"Time (s)\": 165.20011516800014, \"type\": \"concurrent\"}, {\"store_sk\": \"14\", \"table name\": \"Total\", \"Time (s)\": 217.59010785199962, \"type\": \"concurrent\"}, {\"store_sk\": \"406\", \"table name\": \"Total\", \"Time (s)\": 335.90167387600013, \"type\": \"concurrent\"}, {\"store_sk\": \"178\", \"table name\": \"Total\", \"Time (s)\": 708.3831757899998, \"type\": \"concurrent\"}, {\"store_sk\": \"529\", \"table name\": \"Total\", \"Time (s)\": 301.4059971890006, \"type\": \"sequential\"}, {\"store_sk\": \"650\", \"table name\": \"Total\", \"Time (s)\": 207.10258764499986, \"type\": \"sequential\"}, {\"store_sk\": \"14\", \"table name\": \"Total\", \"Time (s)\": 255.96043815599978, \"type\": \"sequential\"}, {\"store_sk\": \"406\", \"table name\": \"Total\", \"Time (s)\": 440.84587619700005, \"type\": \"sequential\"}, {\"store_sk\": \"178\", \"table name\": \"Total\", \"Time (s)\": 840.4732947220004, \"type\": \"sequential\"}]}}, {\"mode\": \"vega-lite\"});\n</script>"]}}],"execution_count":34},{"cell_type":"markdown","source":["The above chart shows how concurrent execution fairs against sequential execution. This is done by adding the timing for each table for the last run to emulate the result had the last execution ran sequentially.\n\nTime diference between concurrent and sequential execution may not look big but that is because the `store_returns` table has very less record as compared with `store_sales` to make any significant difference. However in practise this scales much better with more number of tables. In my project I had to insert data into 26 tables and using the similar concurrency appraoch I was able to reduce the time by over 70%. An important point to note here is degree of parallelization will depends upon the cluster capacity. With more tables you will need more number of workers in the cluster. Experiment with different numbers to find sweet spot of best performance vs cost ratio for your use case.\n\nIf you have any questions leave it a comment below. In next post we will discuss how bulk loading performs against different indexing strategy and benchmark them."],"metadata":{}},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":36}],"metadata":{"name":"01","notebookId":3835927987090890,"kernelspec":{"name":"python_defaultSpec_1598857303332","display_name":"Python 3.7.3 64-bit ('dbconnect6.6': conda)"}},"nbformat":4,"nbformat_minor":0}