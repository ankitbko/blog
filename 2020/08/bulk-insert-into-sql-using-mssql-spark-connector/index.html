<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Microsoft SQL Spark connector | F5</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Microsoft SQL Spark connector" />
<meta name="author" content="Ankit Sinha" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Awesome summary" />
<meta property="og:description" content="Awesome summary" />
<link rel="canonical" href="https://ankitbko.github.io/blog/2020/08/bulk-insert-into-sql-using-mssql-spark-connector/" />
<meta property="og:url" content="https://ankitbko.github.io/blog/2020/08/bulk-insert-into-sql-using-mssql-spark-connector/" />
<meta property="og:site_name" content="F5" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-08-31T00:00:00-05:00" />
<script type="application/ld+json">
{"headline":"Microsoft SQL Spark connector","dateModified":"2020-08-31T00:00:00-05:00","datePublished":"2020-08-31T00:00:00-05:00","author":{"@type":"Person","name":"Ankit Sinha"},"description":"Awesome summary","mainEntityOfPage":{"@type":"WebPage","@id":"https://ankitbko.github.io/blog/2020/08/bulk-insert-into-sql-using-mssql-spark-connector/"},"@type":"BlogPosting","url":"https://ankitbko.github.io/blog/2020/08/bulk-insert-into-sql-using-mssql-spark-connector/","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/blog/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://ankitbko.github.io/blog/feed.xml" title="F5" /><!-- the google_analytics_id gets auto inserted from the config file -->



<script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','//www.google-analytics.com/analytics.js','ga');ga('create','UA-75679348-1','auto');ga('require','displayfeatures');ga('send','pageview');</script>

<link rel="shortcut icon" type="image/x-icon" href="/blog/images/favicon.ico"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Microsoft SQL Spark connector | F5</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Microsoft SQL Spark connector" />
<meta name="author" content="Ankit Sinha" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Awesome summary" />
<meta property="og:description" content="Awesome summary" />
<link rel="canonical" href="https://ankitbko.github.io/blog/2020/08/bulk-insert-into-sql-using-mssql-spark-connector/" />
<meta property="og:url" content="https://ankitbko.github.io/blog/2020/08/bulk-insert-into-sql-using-mssql-spark-connector/" />
<meta property="og:site_name" content="F5" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-08-31T00:00:00-05:00" />
<script type="application/ld+json">
{"headline":"Microsoft SQL Spark connector","dateModified":"2020-08-31T00:00:00-05:00","datePublished":"2020-08-31T00:00:00-05:00","author":{"@type":"Person","name":"Ankit Sinha"},"description":"Awesome summary","mainEntityOfPage":{"@type":"WebPage","@id":"https://ankitbko.github.io/blog/2020/08/bulk-insert-into-sql-using-mssql-spark-connector/"},"@type":"BlogPosting","url":"https://ankitbko.github.io/blog/2020/08/bulk-insert-into-sql-using-mssql-spark-connector/","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

<link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
<link rel="stylesheet" href="//use.fontawesome.com/releases/v5.0.7/css/all.css"><link type="application/atom+xml" rel="alternate" href="https://ankitbko.github.io/blog/feed.xml" title="F5" /><!-- the google_analytics_id gets auto inserted from the config file -->



<script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','//www.google-analytics.com/analytics.js','ga');ga('create','UA-75679348-1','auto');ga('require','displayfeatures');ga('send','pageview');</script>


    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"> </script>
    <script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/blog/">F5</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/blog/about/">About Me</a><a class="page-link" href="/blog/search/">Search</a><a class="page-link" href="/blog/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Microsoft SQL Spark connector</h1><p class="page-description">Awesome summary</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2020-08-31T00:00:00-05:00" itemprop="datePublished">
        Aug 31, 2020
      </time>
       â€¢ <span class="read-time" title="Estimated read time">
    
    
      16 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/blog/categories/#Spark">Spark</a>
        &nbsp;
      
        <a class="category-tags-link" href="/blog/categories/#jupyter">jupyter</a>
        
      
      </p>
    

    
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-08-31-bulk-insert-into-sql-using-mssql-spark-connector.ipynb
-->

<div class="container" id="notebook-container">
        
    
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>With rise of big data, polyglot persistance and availability of cheaper storage technology it is becoming a common scenario where organizations prefer to keep data into cheaper long term storage such as ADLS and load them into OLTP or OLAP databases as needed. In this 3 part blog series we will check out newly released <a href="https://github.com/microsoft/sql-spark-connector">Apache Spark Connector for SQL Server and Azure SQL</a> (aka Microsoft SQL Spark Connector) and use it to insert large amount of data into Azure SQL Hyperscale. We will also capture benchmarks and also discuss some common problems we faced and solutions to them.</p>
<p>In this first post we will see to use Microsoft SQL Spark Connector to bulk load data into Azure SQL from Azure Data Lake and how to optimize it even further. In the second part we will capture and compare benchmarks of bulk loading large dataset into different Azure SQL databases each having different indexing strategy. And in the final post we will discuss an issue with deadlocks (that we will face along this journey) and potential solultion. In each of the posts in this series I will mention what environment and dataset I have used and also share direct links to the scripts used.</p>
<p>So lets get started.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Getting-started-with-Microsoft-SQL-Spark-Connector">Getting started with Microsoft SQL Spark Connector<a class="anchor-link" href="#Getting-started-with-Microsoft-SQL-Spark-Connector"> </a></h2><p>Microsoft SQL Spark Connector is an evolution of now deprecated <a href="https://github.com/Azure/azure-sqldb-spark">Azure SQL Spark Connector</a>. It provides hosts of different features to easily integrate with SQL Server and Azure SQL from spark. At the time of writing this blog, the connector is in active development and the package is not yet published to maven repository. So in order to get it you can either download precompiled <em>jar</em> file from the <a href="https://github.com/microsoft/sql-spark-connector/releases">releases</a> tab in repository, or build the master branch locally. In either way once you have the jar file install it into your databricks cluster.</p>
<blockquote><p>As of writing this post, the connector does not support Spark 3.0.</p>
</blockquote>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Environment">Environment<a class="anchor-link" href="#Environment"> </a></h2><p>We will be using Azure Databricks with cluster configurations as following -</p>
<ul>
<li>Cluster Mode: Standard</li>
<li>Databricks Runtime Version: 6.6 (includes Apache Spark 2.4.5, Scala 2.11)</li>
<li>Workers: 2 </li>
<li>Worker Type: Standard_DS3_v2 (14.0 GB Memory, 4 Cores, 0.75 DBU)</li>
<li>Driver Type: Standard_DS3_v2 (14.0 GB Memory, 4 Cores, 0.75 DBU)</li>
</ul>
<p>Libraries installed in the cluster -</p>
<ul>
<li>Microsoft SQL Spark Connector (jar) - Built at <a href="https://github.com/microsoft/sql-spark-connector/tree/c787e8ff854eb356c31092c9815e3a43617d9b4f">c787e8f</a></li>
<li>codetiming (PyPI) - Used to capture metrics</li>
</ul>
<p>Azure SQL Server Hyperscale configured at 2vCore and 0 replicas. In this post we will be using a single database which has tables as per this <a href="">SQL DDL script TODOOOO</a>.</p>
<p>Azure Data Lake Gen 2 contains parquet files for the dataset we use which is then <a href="https://docs.databricks.com/data/data-sources/azure/azure-datalake-gen2.html#mount-an-azure-data-lake-storage-gen2-account-using-a-service-principal-and-oauth-20">mounted</a> on Databricks.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Dataset">Dataset<a class="anchor-link" href="#Dataset"> </a></h2><p>We will be using 1 TB TPC-DS dataset v2.13.0rc1 for this blog series. Due to licensing restriction I cannot include them in my repo so you can download it <a href="http://www.tpc.org/tpcds/default5.asp">here</a>. However I did included SQL DDL statements for creating tables <a href="">here TODOOOOO</a>.</p>
<p>In this post we will be focusing on only 3 tables - store, store_returns and store_sales. As the name suggests store_sales and store_returns contains items sold or returned for different stores. For more information about the dataset refer to the specification document available in the TPC-DS download.</p>
<p>There are no foreign key in the tables so Surrogate Key of store (<code>s_store_sk</code>) will be used to query and group results from store_sales (<code>ss_store_sk</code>) and store_returns (<code>sr_store_sk</code>).</p>
<p>This dataset was already available in my Azure SQL database and is then exported as parquet files using <a href="">this export notebook TODOOOOO</a>. In case you have the dataset as pipe-separated text files you can modify the export notebook accordingly.</p>
<blockquote><p>If you generate parquet files from any text format such as (csv, pipe-separated etc) you will need to correctly specify schema while writing to parquet. You can find the schema in specification document of TPC-DS.</p>
</blockquote>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<style scoped="">
  .ansiout {
    display: block;
    unicode-bidi: embed;
    white-space: pre-wrap;
    word-wrap: break-word;
    word-break: break-all;
    font-family: "Source Code Pro", "Menlo", monospace;;
    font-size: 13px;
    color: #555;
    margin-left: 4px;
    line-height: 19px;
  }
</style>
<div class="ansiout">Out[9]: [&#39;dbfs:/mnt/adls/adls7dataset7benchmark/tpcds1tb/parquet/store_sales/_SUCCESS&#39;,
 &#39;dbfs:/mnt/adls/adls7dataset7benchmark/tpcds1tb/parquet/store_sales/ss_store_sk=1/&#39;,
 &#39;dbfs:/mnt/adls/adls7dataset7benchmark/tpcds1tb/parquet/store_sales/ss_store_sk=10/&#39;,
 &#39;dbfs:/mnt/adls/adls7dataset7benchmark/tpcds1tb/parquet/store_sales/ss_store_sk=100/&#39;,
 &#39;dbfs:/mnt/adls/adls7dataset7benchmark/tpcds1tb/parquet/store_sales/ss_store_sk=1000/&#39;]</div>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>If you notice above the <em>store_sales</em> dataset is partitioned on <code>ss_store_sk</code>. This is a logical way to save data as we will be filtering on <code>ss_store_sk</code> and only work with few of the stores. This is also set us up for an issue that we will face later. The same partitioning strategy is used for <em>store_returns</em> as well.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Bulk-Loading-data-into-Azure-SQL-Server">Bulk Loading data into Azure SQL Server<a class="anchor-link" href="#Bulk-Loading-data-into-Azure-SQL-Server"> </a></h2><p>Our use case will be to load store related data (store_sales, store_returns) into Azure SQL for different store ids.</p>
<p>To compare results we will capture the time it takes to insert records into each table. Therefore there are some boilerplace code that captures metrics which can be safely removed. Anything within <code># --- Capturing Metrics Start---</code> and <code># --- Capturing Metrics End---</code> block is used solely for capturing metrics and can be safely ignored. It does not have any impact on importing data into Azure SQL.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">codetiming</span> <span class="kn">import</span> <span class="n">Timer</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="n">path</span> <span class="o">=</span> <span class="s2">&quot;/mnt/adls/adls7dataset7benchmark/tpcds1tb/parquet/&quot;</span>
<span class="n">server_name</span> <span class="o">=</span> <span class="n">dbutils</span><span class="o">.</span><span class="n">secrets</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">scope</span> <span class="o">=</span> <span class="s2">&quot;kvbenchmark&quot;</span><span class="p">,</span> <span class="n">key</span> <span class="o">=</span> <span class="s2">&quot;db-server-url&quot;</span><span class="p">)</span>
<span class="n">username</span> <span class="o">=</span> <span class="n">dbutils</span><span class="o">.</span><span class="n">secrets</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">scope</span> <span class="o">=</span> <span class="s2">&quot;kvbenchmark&quot;</span><span class="p">,</span> <span class="n">key</span> <span class="o">=</span> <span class="s2">&quot;db-username&quot;</span><span class="p">)</span>
<span class="n">password</span> <span class="o">=</span> <span class="n">dbutils</span><span class="o">.</span><span class="n">secrets</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">scope</span> <span class="o">=</span> <span class="s2">&quot;kvbenchmark&quot;</span><span class="p">,</span> <span class="n">key</span> <span class="o">=</span> <span class="s2">&quot;db-password&quot;</span><span class="p">)</span>
<span class="n">schema</span> <span class="o">=</span> <span class="s2">&quot;dbo&quot;</span>

<span class="n">database_name</span> <span class="o">=</span> <span class="s2">&quot;noidx&quot;</span>
<span class="n">url</span> <span class="o">=</span> <span class="n">server_name</span> <span class="o">+</span> <span class="s2">&quot;;&quot;</span> <span class="o">+</span> <span class="s2">&quot;databaseName=&quot;</span> <span class="o">+</span> <span class="n">database_name</span> <span class="o">+</span> <span class="s2">&quot;;&quot;</span>
<span class="n">url</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<style scoped="">
  .ansiout {
    display: block;
    unicode-bidi: embed;
    white-space: pre-wrap;
    word-wrap: break-word;
    word-break: break-all;
    font-family: "Source Code Pro", "Menlo", monospace;;
    font-size: 13px;
    color: #555;
    margin-left: 4px;
    line-height: 19px;
  }
</style>
<div class="ansiout">Out[15]: &#39;[REDACTED];databaseName=noidx;&#39;</div>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Here I start with importing relevant packages and getting secrets from our secret-store. In case you want to replicate this you will need to modify the above cell accordingly. I am using <code>codetiming</code> and <code>pandas</code> packages to capture metrics and is not needed for bulk importing itself.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">pyspark.sql.types</span> <span class="kn">import</span> <span class="o">*</span>

<span class="c1"># List of table names</span>
<span class="n">tables</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;store&quot;</span><span class="p">,</span> <span class="s2">&quot;store_returns&quot;</span><span class="p">,</span> <span class="s2">&quot;store_sales&quot;</span><span class="p">]</span>

<span class="c1"># Map between table names and store surrogate key</span>
<span class="n">table_storesk_map</span> <span class="o">=</span> <span class="p">{</span>
  <span class="s2">&quot;store&quot;</span><span class="p">:</span> <span class="s2">&quot;s_store_sk&quot;</span><span class="p">,</span> 
  <span class="s2">&quot;store_returns&quot;</span><span class="p">:</span> <span class="s2">&quot;sr_store_sk&quot;</span><span class="p">,</span>
  <span class="s2">&quot;store_sales&quot;</span><span class="p">:</span> <span class="s2">&quot;ss_store_sk&quot;</span>
<span class="p">}</span>

<span class="c1"># Map between table names and schema</span>
<span class="n">table_schema_map</span> <span class="o">=</span> <span class="p">{</span>
  <span class="s2">&quot;store&quot;</span><span class="p">:</span> <span class="p">[</span>
    <span class="n">StructField</span><span class="p">(</span><span class="s2">&quot;s_store_sk&quot;</span><span class="p">,</span> <span class="n">IntegerType</span><span class="p">(),</span> <span class="kc">False</span><span class="p">),</span>
    <span class="n">StructField</span><span class="p">(</span><span class="s2">&quot;s_store_id&quot;</span><span class="p">,</span> <span class="n">StringType</span><span class="p">(),</span> <span class="kc">False</span><span class="p">)</span>
  <span class="p">],</span>
  <span class="s2">&quot;store_returns&quot;</span><span class="p">:</span> <span class="p">[</span>
    <span class="n">StructField</span><span class="p">(</span><span class="s2">&quot;sr_item_sk&quot;</span><span class="p">,</span> <span class="n">IntegerType</span><span class="p">(),</span> <span class="kc">False</span><span class="p">),</span> 
    <span class="n">StructField</span><span class="p">(</span><span class="s2">&quot;sr_ticket_number&quot;</span><span class="p">,</span> <span class="n">IntegerType</span><span class="p">(),</span> <span class="kc">False</span><span class="p">),</span>
  <span class="p">],</span>
  <span class="s2">&quot;store_sales&quot;</span><span class="p">:</span> <span class="p">[</span>
    <span class="n">StructField</span><span class="p">(</span><span class="s2">&quot;ss_item_sk&quot;</span><span class="p">,</span> <span class="n">IntegerType</span><span class="p">(),</span> <span class="kc">False</span><span class="p">),</span> 
    <span class="n">StructField</span><span class="p">(</span><span class="s2">&quot;ss_ticket_number&quot;</span><span class="p">,</span> <span class="n">IntegerType</span><span class="p">(),</span> <span class="kc">False</span><span class="p">)</span>
  <span class="p">]</span>
<span class="p">}</span>

<span class="c1"># List of stores that we use</span>
<span class="n">stores</span> <span class="o">=</span> <span class="p">[</span><span class="n">row</span><span class="p">[</span><span class="s2">&quot;ss_store_sk&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">parquet</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">path</span><span class="si">}</span><span class="s2">/store_sales&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">filter</span><span class="p">(</span><span class="s2">&quot;ss_store_sk IS NOT null&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">groupBy</span><span class="p">(</span><span class="s1">&#39;ss_store_sk&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">count</span><span class="p">()</span><span class="o">.</span><span class="n">orderBy</span><span class="p">(</span><span class="s1">&#39;count&#39;</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="s2">&quot;ss_store_sk&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="mi">5</span><span class="p">)]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">stores</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<style scoped="">
  .ansiout {
    display: block;
    unicode-bidi: embed;
    white-space: pre-wrap;
    word-wrap: break-word;
    word-break: break-all;
    font-family: "Source Code Pro", "Menlo", monospace;;
    font-size: 13px;
    color: #555;
    margin-left: 4px;
    line-height: 19px;
  }
</style>
<div class="ansiout">[529, 650, 14, 406, 178]
</div>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Next we initialize the tables and stores that we will use for importing into Azure SQL. <code>table_schema_map</code> is used to correct the schema to match that of database. This is a temporary workaround for <a href="https://github.com/microsoft/sql-spark-connector/issues/5">issue #5</a>. We will be bulk insert data in store, store_sales and store_returns for 5 stores having highest number of records in <em>store_sales</em>. The count of records for each of the stores are as below. Overall we will be inserting ~30 million records into our database.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>store</th>
      <th>store_returns</th>
      <th>store_sales</th>
      <th>Total</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>529</th>
      <td>1</td>
      <td>555343</td>
      <td>5512441</td>
      <td>6067785</td>
    </tr>
    <tr>
      <th>650</th>
      <td>1</td>
      <td>555966</td>
      <td>5510505</td>
      <td>6066472</td>
    </tr>
    <tr>
      <th>14</th>
      <td>1</td>
      <td>554904</td>
      <td>5508259</td>
      <td>6063164</td>
    </tr>
    <tr>
      <th>406</th>
      <td>1</td>
      <td>554828</td>
      <td>5506912</td>
      <td>6061741</td>
    </tr>
    <tr>
      <th>178</th>
      <td>1</td>
      <td>554147</td>
      <td>5506321</td>
      <td>6060469</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>I have created a method to truncate the tables that we are working with. This will make it easier for us to rerun any of tests without getting into trouble with duplicate Primary key.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">truncate_tables</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">username</span><span class="p">,</span> <span class="n">password</span><span class="p">,</span> <span class="n">tables</span><span class="p">):</span>
  <span class="k">for</span> <span class="n">table</span> <span class="ow">in</span> <span class="n">tables</span><span class="p">:</span>
    <span class="n">query</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;TRUNCATE TABLE </span><span class="si">{</span><span class="n">schema</span><span class="si">}</span><span class="s2">.</span><span class="si">{</span><span class="n">table</span><span class="si">}</span><span class="s2">&quot;</span>

    <span class="k">try</span><span class="p">:</span>
      <span class="n">t</span> <span class="o">=</span> <span class="n">Timer</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;Tuncated table </span><span class="si">{</span><span class="n">table</span><span class="si">}</span><span class="s2"> in: </span><span class="se">{{</span><span class="s2">:0.2f</span><span class="se">}}</span><span class="s2">&quot;</span><span class="p">)</span>
      <span class="n">t</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>
      <span class="n">driver_manager</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">_sc</span><span class="o">.</span><span class="n">_gateway</span><span class="o">.</span><span class="n">jvm</span><span class="o">.</span><span class="n">java</span><span class="o">.</span><span class="n">sql</span><span class="o">.</span><span class="n">DriverManager</span>
      <span class="n">con</span> <span class="o">=</span> <span class="n">driver_manager</span><span class="o">.</span><span class="n">getConnection</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">username</span><span class="p">,</span> <span class="n">password</span><span class="p">)</span>

      <span class="n">stmt</span> <span class="o">=</span> <span class="n">con</span><span class="o">.</span><span class="n">createStatement</span><span class="p">()</span>
      <span class="n">stmt</span><span class="o">.</span><span class="n">executeUpdate</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
      <span class="n">stmt</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
      <span class="n">t</span><span class="o">.</span><span class="n">stop</span><span class="p">()</span>
    <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
      <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Failed to truncate table </span><span class="si">{</span><span class="n">table</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">e</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<style scoped="">
  .ansiout {
    display: block;
    unicode-bidi: embed;
    white-space: pre-wrap;
    word-wrap: break-word;
    word-break: break-all;
    font-family: "Source Code Pro", "Menlo", monospace;;
    font-size: 13px;
    color: #555;
    margin-left: 4px;
    line-height: 19px;
  }
</style>
<div class="ansiout"></div>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Inserting-into-each-table-sequentially">Inserting into each table sequentially<a class="anchor-link" href="#Inserting-into-each-table-sequentially"> </a></h3>
</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<style scoped="">
  .ansiout {
    display: block;
    unicode-bidi: embed;
    white-space: pre-wrap;
    word-wrap: break-word;
    word-break: break-all;
    font-family: "Source Code Pro", "Menlo", monospace;;
    font-size: 13px;
    color: #555;
    margin-left: 4px;
    line-height: 19px;
  }
</style>
<div class="ansiout"></div>
</div>

</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Temporary workaround until Issue #5 gets fixed https://github.com/microsoft/sql-spark-connector/issues/5</span>
<span class="k">def</span> <span class="nf">create_valid_table_schema</span><span class="p">(</span><span class="n">table_name</span><span class="p">,</span> <span class="n">df_schema</span><span class="p">):</span> 
  <span class="k">return</span> <span class="n">StructType</span><span class="p">([</span><span class="n">x</span> <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="nb">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">n</span><span class="p">:</span> <span class="n">n</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">table_schema_map</span><span class="p">[</span><span class="n">table_name</span><span class="p">])</span> <span class="k">else</span> <span class="nb">next</span><span class="p">(</span><span class="nb">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">f</span><span class="p">:</span> <span class="n">f</span><span class="o">.</span><span class="n">name</span> <span class="o">==</span> <span class="n">x</span><span class="o">.</span><span class="n">name</span> <span class="p">,</span><span class="n">table_schema_map</span><span class="p">[</span><span class="n">table_name</span><span class="p">]))</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">df_schema</span><span class="p">])</span>

<span class="k">def</span> <span class="nf">import_single_store</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">table_name</span><span class="p">,</span> <span class="n">store_sk</span><span class="p">):</span> 
  <span class="k">try</span><span class="p">:</span>
      <span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">parquet</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">path</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">table_name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
      <span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">table_storesk_map</span><span class="p">[</span><span class="n">table_name</span><span class="p">]</span><span class="si">}</span><span class="s2"> == </span><span class="si">{</span><span class="n">store_sk</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

      <span class="c1"># Temporary workaround until Issue #5 gets fixed https://github.com/microsoft/sql-spark-connector/issues/5</span>
      <span class="n">table_schema</span> <span class="o">=</span> <span class="n">create_valid_table_schema</span><span class="p">(</span><span class="n">table_name</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">schema</span><span class="p">)</span>
      <span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">rdd</span><span class="p">,</span> <span class="n">table_schema</span><span class="p">)</span>

      <span class="n">t</span> <span class="o">=</span> <span class="n">Timer</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;Imported store </span><span class="si">{</span><span class="n">store_sk</span><span class="si">}</span><span class="s2"> into table </span><span class="si">{</span><span class="n">table_name</span><span class="si">}</span><span class="s2"> in : </span><span class="se">{{</span><span class="s2">:0.2f</span><span class="se">}}</span><span class="s2"> &quot;</span><span class="p">)</span>    
      <span class="n">t</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>
      
      <span class="n">df</span><span class="o">.</span><span class="n">write</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s2">&quot;com.microsoft.sqlserver.jdbc.spark&quot;</span><span class="p">)</span> \
        <span class="o">.</span><span class="n">mode</span><span class="p">(</span><span class="s2">&quot;append&quot;</span><span class="p">)</span> \
        <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&quot;url&quot;</span><span class="p">,</span> <span class="n">url</span><span class="p">)</span> \
        <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&quot;dbtable&quot;</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">schema</span><span class="si">}</span><span class="s2">.</span><span class="si">{</span><span class="n">table_name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span> \
        <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="n">username</span><span class="p">)</span> \
        <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&quot;password&quot;</span><span class="p">,</span> <span class="n">password</span><span class="p">)</span> \
        <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&quot;tableLock&quot;</span><span class="p">,</span> <span class="s2">&quot;false&quot;</span><span class="p">)</span> \
        <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&quot;batchsize&quot;</span><span class="p">,</span> <span class="s2">&quot;100000&quot;</span><span class="p">)</span> \
        <span class="o">.</span><span class="n">save</span><span class="p">()</span>

      <span class="n">elapsed</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">stop</span><span class="p">()</span>
      <span class="k">return</span> <span class="n">elapsed</span>
  <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Failed to import </span><span class="si">{</span><span class="n">store_sk</span><span class="si">}</span><span class="s2"> into table </span><span class="si">{</span><span class="n">table_name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">e</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<style scoped="">
  .ansiout {
    display: block;
    unicode-bidi: embed;
    white-space: pre-wrap;
    word-wrap: break-word;
    word-break: break-all;
    font-family: "Source Code Pro", "Menlo", monospace;;
    font-size: 13px;
    color: #555;
    margin-left: 4px;
    line-height: 19px;
  }
</style>
<div class="ansiout"></div>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Here we have two methods which will be reused throught this blog. <code>create_valid_table_schema</code> method takes the schema of the parquet files and merges it with schema defined in <code>table_schema_map</code>. This is a temporary fix as discussed before.</p>
<p><code>import_single_store(url, table_name, store_sk)</code> takes in the url of the database, the table name and the surrogate key (<code>store_sk</code>) of the store. It then reads the parquet file for the specified table filtered by <code>store_sk</code>. It then corrects the schema, starts the timer and submits the inerstion job to spark. The two parameters <code>tableLock</code> and <code>batchsize</code> are crucial.</p>
<ul>
<li><code>tableLock</code> or <a href="https://docs.microsoft.com/en-us/sql/t-sql/queries/hints-transact-sql-table?view=sql-server-ver15">TABLOCK</a> specifies that the acquired lock is applied at the table level. This option is helpful when inserting into Heap table.</li>
<li><code>batchsize</code> describes how many rows are committed at a time during the bulk operation. Play around with it to see how the system performs with different batch sizes. This option will come useful later when we bulk insert into a Clustered Columnstore Index (CCI).</li>
</ul>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">import_stores</span><span class="p">(</span><span class="n">url</span><span class="p">):</span>
  <span class="k">for</span> <span class="n">store_sk</span> <span class="ow">in</span> <span class="n">stores</span><span class="p">:</span>      
      <span class="c1"># -------- Capturing Metrics Start---------</span>
      <span class="n">t</span> <span class="o">=</span> <span class="n">Timer</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;Imported store </span><span class="si">{</span><span class="n">store_sk</span><span class="si">}</span><span class="s2"> in : </span><span class="se">{{</span><span class="s2">:0.2f</span><span class="se">}}</span><span class="s2">&quot;</span><span class="p">)</span>
      <span class="n">t</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>
      <span class="n">metrics_row</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;store_sk&#39;</span><span class="p">:</span> <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">store_sk</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">}</span>
      <span class="c1"># -------- Capturing Metrics End---------</span>
      
      <span class="k">for</span> <span class="n">table_name</span> <span class="ow">in</span> <span class="n">table_storesk_map</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
        <span class="n">elapsed</span> <span class="o">=</span> <span class="n">import_single_store</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">table_name</span><span class="p">,</span> <span class="n">store_sk</span><span class="p">)</span>

        <span class="c1"># -------- Capturing Metrics Start---------</span>
        <span class="n">metrics_row</span><span class="p">[</span><span class="n">table_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">elapsed</span>
      
      <span class="n">elapsed</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">stop</span><span class="p">()</span>
      <span class="n">metrics_row</span><span class="p">[</span><span class="s1">&#39;Total&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">elapsed</span>
      <span class="k">global</span> <span class="n">metrics_seq_df</span>
      <span class="n">metrics_seq_df</span> <span class="o">=</span> <span class="n">metrics_seq_df</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">metrics_row</span><span class="p">,</span> <span class="n">ignore_index</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
      <span class="c1"># -------- Capturing Metrics End---------</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<style scoped="">
  .ansiout {
    display: block;
    unicode-bidi: embed;
    white-space: pre-wrap;
    word-wrap: break-word;
    word-break: break-all;
    font-family: "Source Code Pro", "Menlo", monospace;;
    font-size: 13px;
    color: #555;
    margin-left: 4px;
    line-height: 19px;
  }
</style>
<div class="ansiout"></div>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><code>import_stores</code> loops over all stores and calls <code>import_single_store</code> for each table. Note that it waits for <code>import_single_store</code> to complete before continuing the loop and inserting into next table. It is not most optimized way to import into bunch of tables and we will see how we can improve it in just a bit. For now lets truncate the tables and run it to capture the reults.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">truncate_tables</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">username</span><span class="p">,</span> <span class="n">password</span><span class="p">,</span> <span class="n">tables</span><span class="p">)</span>
<span class="n">import_stores</span><span class="p">(</span><span class="n">url</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<style scoped="">
  .ansiout {
    display: block;
    unicode-bidi: embed;
    white-space: pre-wrap;
    word-wrap: break-word;
    word-break: break-all;
    font-family: "Source Code Pro", "Menlo", monospace;;
    font-size: 13px;
    color: #555;
    margin-left: 4px;
    line-height: 19px;
  }
</style>
<div class="ansiout">Tuncated table store in: 0.09
Tuncated table store_returns in: 0.03
Tuncated table store_sales in: 0.03
Imported store 529 into table store in : 0.33 
Imported store 529 into table store_returns in : 26.75 
Imported store 529 into table store_sales in : 284.44 
Imported store 529 in : 316.37
Imported store 650 into table store in : 0.39 
Imported store 650 into table store_returns in : 28.09 
Imported store 650 into table store_sales in : 150.40 
Imported store 650 in : 183.74
Imported store 14 into table store in : 0.39 
Imported store 14 into table store_returns in : 27.87 
Imported store 14 into table store_sales in : 162.81 
Imported store 14 in : 195.93
Imported store 406 into table store in : 0.31 
Imported store 406 into table store_returns in : 28.95 
Imported store 406 into table store_sales in : 337.09 
Imported store 406 in : 371.61
Imported store 178 into table store in : 0.26 
Imported store 178 into table store_returns in : 47.05 
Imported store 178 into table store_sales in : 599.09 
Imported store 178 in : 651.47
</div>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The import is going to succeed and the timings will be logged for each table.</p>
<p>There are two interesting thing that you may notice when you run this. First is that only one spark job runs at a time. This is because we are inserting into one table at a time and the spark will create only 1 job for each <code>spark.write</code> when inserting data into Azure SQL. This is important fact to keep in mind to optimize in future.</p>
<p><img src="/blog/images/copied_from_nb/./assets/images/spark_jobs.png" alt="spark_job" /></p>
<p>The second is a little tricky to find. If you check the stages of running job when it inserts into <code>store_sales</code> table in Spark UI you will notice some tasks will fail due to Deadlock. 
<code>com.microsoft.sqlserver.jdbc.SQLServerException: Transaction (Process ID 99) was deadlocked on lock resources with another process and has been chosen as the deadlock victim. Rerun the transaction.</code></p>
<p>Although these tasks failed the stage and job overall succeeded and this is due to retry mechanism within Databricks. You can confirm everything worked by counting number of records in SQL. We let it be for now as we will be discussing the root cause of deadlock and potential solutions in part 3 of this blog.</p>
<p><img src="/blog/images/copied_from_nb/./assets/images/deadlock.png" alt="spark_job" /></p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>store_sk</th>
      <th>store</th>
      <th>store_returns</th>
      <th>store_sales</th>
      <th>Total</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>529</td>
      <td>0.329149</td>
      <td>26.747128</td>
      <td>284.442049</td>
      <td>316.367564</td>
    </tr>
    <tr>
      <th>2</th>
      <td>650</td>
      <td>0.387246</td>
      <td>28.089986</td>
      <td>150.395265</td>
      <td>183.738926</td>
    </tr>
    <tr>
      <th>3</th>
      <td>14</td>
      <td>0.386457</td>
      <td>27.867441</td>
      <td>162.805537</td>
      <td>195.926244</td>
    </tr>
    <tr>
      <th>4</th>
      <td>406</td>
      <td>0.311108</td>
      <td>28.947312</td>
      <td>337.091057</td>
      <td>371.607109</td>
    </tr>
    <tr>
      <th>5</th>
      <td>178</td>
      <td>0.256431</td>
      <td>47.049702</td>
      <td>599.092014</td>
      <td>651.472770</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>If we look at the timings it becomes obvious that overall time it took to insert all the data related to single store is sum of time it took to insert into each of the tables for that store. But can we do any better? Ususally in most system partially inserted data is not of use so we are delayed till the data is inserted on all related tables. So our next objective is to reduce the time it takes to insert all data for a single store. Wouldn't it be great if we can insert data for single store in all the tables concurrently?</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Parallelizing-insertion-of-data-in-all-tables">Parallelizing insertion of data in all tables<a class="anchor-link" href="#Parallelizing-insertion-of-data-in-all-tables"> </a></h3><p>To achieve this we will utilize Python's multithreading library to submit multiple spark jobs concurrently. As you know anything we write and execute through databricks notebook runs on executor. And when executor find statement related to spark, it submits the job to spark and spark then orchestrates its execution on the workers. The observation from above tells us that only single job (and most of the time single stage) is executing in our cluster at any moment this means or cluster is woefully underutilized. Our plan is to submit multiple spark jobs to better utilize our cluster.</p>
<p>To make it work properly with python we have to set <code>PYSPARK_PIN_THREAD</code> environment variable in python and set it to <code>true</code>. You need to do this as part of cluster startup. You can find more details about it <a href="https://spark.apache.org/docs/2.4.5/job-scheduling.html#concurrent-jobs-in-pyspark">here</a>. Note that as of yet this flag is not recommended in production. In case you want to perform such parallelization its better to use Scala.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">threading</span>

<span class="k">class</span> <span class="nc">myThread</span> <span class="p">(</span><span class="n">threading</span><span class="o">.</span><span class="n">Thread</span><span class="p">):</span>
   <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">table_name</span><span class="p">,</span> <span class="n">store_sk</span><span class="p">):</span>
      <span class="n">threading</span><span class="o">.</span><span class="n">Thread</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">table_name</span> <span class="o">=</span> <span class="n">table_name</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">store_sk</span> <span class="o">=</span> <span class="n">store_sk</span>
  
   <span class="k">def</span> <span class="nf">run</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>      
      <span class="n">elapsed</span> <span class="o">=</span> <span class="n">import_single_store</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">table_name</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">store_sk</span><span class="p">)</span>

      <span class="c1"># -------- Capturing Metrics Start---------</span>
      <span class="k">global</span> <span class="n">metrics_con_df</span>
      <span class="n">metrics_con_df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">metrics_con_df</span><span class="p">[</span><span class="s1">&#39;store_sk&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">store_sk</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">table_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">elapsed</span>
      <span class="c1"># -------- Capturing Metrics End---------</span>


<span class="k">def</span> <span class="nf">import_stores_concurrent</span><span class="p">():</span>
  <span class="k">for</span> <span class="n">store_sk</span> <span class="ow">in</span> <span class="n">stores</span><span class="p">:</span>
      
      <span class="c1"># -------- Capturing Metrics Start---------</span>
      <span class="n">time</span> <span class="o">=</span> <span class="n">Timer</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;Imported store </span><span class="si">{</span><span class="n">store_sk</span><span class="si">}</span><span class="s2"> in : </span><span class="se">{{</span><span class="s2">:0.2f</span><span class="se">}}</span><span class="s2">&quot;</span><span class="p">)</span>
      <span class="n">time</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>
      
      <span class="k">global</span> <span class="n">metrics_con_df</span>
      <span class="n">metrics_con_df</span> <span class="o">=</span> <span class="n">metrics_con_df</span><span class="o">.</span><span class="n">append</span><span class="p">({</span><span class="s1">&#39;store_sk&#39;</span><span class="p">:</span> <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">store_sk</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">},</span> <span class="n">ignore_index</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
      <span class="c1"># -------- Capturing Metrics End---------</span>
            
      <span class="n">threads</span> <span class="o">=</span> <span class="p">[]</span>
      
      <span class="k">for</span> <span class="n">table_name</span> <span class="ow">in</span> <span class="n">tables</span><span class="p">:</span>
          <span class="n">thread</span> <span class="o">=</span> <span class="n">myThread</span><span class="p">(</span><span class="n">table_name</span><span class="p">,</span> <span class="n">store_sk</span><span class="p">)</span>
          <span class="n">thread</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>
          <span class="n">threads</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">thread</span><span class="p">)</span>

      <span class="c1"># Wait for all threads to complete</span>
      <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">threads</span><span class="p">:</span>
          <span class="n">t</span><span class="o">.</span><span class="n">join</span><span class="p">()</span>
      
      <span class="c1"># -------- Capturing Metrics Start--------- </span>
      <span class="n">elapsed</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">stop</span><span class="p">()</span>
      <span class="n">metrics_con_df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">metrics_con_df</span><span class="p">[</span><span class="s1">&#39;store_sk&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">store_sk</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="s1">&#39;Total&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">elapsed</span>
      <span class="c1"># -------- Capturing Metrics End---------</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<style scoped="">
  .ansiout {
    display: block;
    unicode-bidi: embed;
    white-space: pre-wrap;
    word-wrap: break-word;
    word-break: break-all;
    font-family: "Source Code Pro", "Menlo", monospace;;
    font-size: 13px;
    color: #555;
    margin-left: 4px;
    line-height: 19px;
  }
</style>
<div class="ansiout"></div>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We start with creating a class with most creative name ever - <code>myThread</code>. Each of myThread will execute <code>import_single_store</code> for a single table for a store. In <code>import_stores_concurrent</code> we loop through the stores and for each table we create a new  <code>myThread</code> that executes <code>import_single_store</code>. Each of this thread is going to submit a job to spark and we will let spark handle the orchestration of our job in our cluster. After submitting all the jobs we wait for all the threads to finish before continuing with next store in loop.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">truncate_tables</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">username</span><span class="p">,</span> <span class="n">password</span><span class="p">,</span> <span class="n">tables</span><span class="p">)</span>
<span class="n">import_stores_concurrent</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<style scoped="">
  .ansiout {
    display: block;
    unicode-bidi: embed;
    white-space: pre-wrap;
    word-wrap: break-word;
    word-break: break-all;
    font-family: "Source Code Pro", "Menlo", monospace;;
    font-size: 13px;
    color: #555;
    margin-left: 4px;
    line-height: 19px;
  }
</style>
<div class="ansiout">Tuncated table store in: 0.12
Tuncated table store_returns in: 0.04
Tuncated table store_sales in: 0.04
Imported store 529 into table store in : 0.82 
Imported store 529 into table store_returns in : 27.54 
Imported store 529 into table store_sales in : 286.87 
Imported store 529 in : 291.06
Imported store 650 into table store in : 3.31 
Imported store 650 into table store_returns in : 40.14 
Imported store 650 into table store_sales in : 163.92 
Imported store 650 in : 167.72
Imported store 14 into table store in : 0.22 
Imported store 14 into table store_returns in : 45.55 
Imported store 14 into table store_sales in : 211.10 
Imported store 14 in : 215.02
Imported store 406 into table store in : 3.10 
Imported store 406 into table store_returns in : 104.55 
Imported store 406 into table store_sales in : 357.15 
Imported store 406 in : 360.78
Imported store 178 into table store in : 0.31 
Imported store 178 into table store_returns in : 132.77 
Imported store 178 into table store_sales in : 639.66 
Imported store 178 in : 643.35
</div>
</div>

</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>store_sk</th>
      <th>store</th>
      <th>store_returns</th>
      <th>store_sales</th>
      <th>Total</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>529</td>
      <td>0.823838</td>
      <td>27.5421</td>
      <td>286.874</td>
      <td>291.062</td>
    </tr>
    <tr>
      <th>1</th>
      <td>650</td>
      <td>3.31197</td>
      <td>40.1433</td>
      <td>163.921</td>
      <td>167.716</td>
    </tr>
    <tr>
      <th>2</th>
      <td>14</td>
      <td>0.219465</td>
      <td>45.5477</td>
      <td>211.102</td>
      <td>215.017</td>
    </tr>
    <tr>
      <th>3</th>
      <td>406</td>
      <td>3.10265</td>
      <td>104.546</td>
      <td>357.153</td>
      <td>360.784</td>
    </tr>
    <tr>
      <th>4</th>
      <td>178</td>
      <td>0.314621</td>
      <td>132.773</td>
      <td>639.663</td>
      <td>643.35</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This time you will notice that multiple spark jobs are running at same time. This becomes more adamant once we look at the captured timings. The total time take for a single store is no more sum of each table but rather MAX for all 3 tables which in our case is <code>store_sales</code>. Now there may be some additional cost involved while scheduling jobs therefore the timings does not match exactly with the <code>store_sales</code>. However thing to note here is that the time taken to insert into <code>store_returns</code> is completely absorbed by the timing of <code>store_sales</code>.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">

<div id="altair-viz-a3eb69d684564f709e59610af6520147"></div>
<script type="text/javascript">
  (function(spec, embedOpt){
    let outputDiv = document.currentScript.previousElementSibling;
    if (outputDiv.id !== "altair-viz-a3eb69d684564f709e59610af6520147") {
      outputDiv = document.getElementById("altair-viz-a3eb69d684564f709e59610af6520147");
    }
    const paths = {
      "vega": "https://cdn.jsdelivr.net/npm//vega@5?noext",
      "vega-lib": "https://cdn.jsdelivr.net/npm//vega-lib?noext",
      "vega-lite": "https://cdn.jsdelivr.net/npm//vega-lite@4.8.1?noext",
      "vega-embed": "https://cdn.jsdelivr.net/npm//vega-embed@6?noext",
    };

    function loadScript(lib) {
      return new Promise(function(resolve, reject) {
        var s = document.createElement('script');
        s.src = paths[lib];
        s.async = true;
        s.onload = () => resolve(paths[lib]);
        s.onerror = () => reject(`Error loading script: ${paths[lib]}`);
        document.getElementsByTagName("head")[0].appendChild(s);
      });
    }

    function showError(err) {
      outputDiv.innerHTML = `<div class="error" style="color:red;">${err}</div>`;
      throw err;
    }

    function displayChart(vegaEmbed) {
      vegaEmbed(outputDiv, spec, embedOpt)
        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));
    }

    if(typeof define === "function" && define.amd) {
      requirejs.config({paths});
      require(["vega-embed"], displayChart, err => showError(`Error loading script: ${err.message}`));
    } else if (typeof vegaEmbed === "function") {
      displayChart(vegaEmbed);
    } else {
      loadScript("vega")
        .then(() => loadScript("vega-lite"))
        .then(() => loadScript("vega-embed"))
        .catch(showError)
        .then(() => displayChart(vegaEmbed));
    }
  })({"config": {"view": {"continuousWidth": 400, "continuousHeight": 300}}, "data": {"name": "data-a9b287f9bca3287cdaf5708a15cc576a"}, "mark": "bar", "encoding": {"color": {"type": "nominal", "field": "type"}, "column": {"type": "nominal", "field": "store_sk"}, "tooltip": [{"type": "nominal", "field": "store_sk"}, {"type": "nominal", "field": "type"}, {"type": "quantitative", "field": "Time (s)"}], "x": {"type": "nominal", "field": "type", "sort": {"field": "type", "order": "descending"}}, "y": {"type": "quantitative", "field": "Time (s)"}}, "$schema": "https://vega.github.io/schema/vega-lite/v4.8.1.json", "datasets": {"data-a9b287f9bca3287cdaf5708a15cc576a": [{"store_sk": "529", "table name": "Total", "Time (s)": 291.06161118900127, "type": "concurrent"}, {"store_sk": "650", "table name": "Total", "Time (s)": 167.71557973199742, "type": "concurrent"}, {"store_sk": "14", "table name": "Total", "Time (s)": 215.01723806899827, "type": "concurrent"}, {"store_sk": "406", "table name": "Total", "Time (s)": 360.78360546099793, "type": "concurrent"}, {"store_sk": "178", "table name": "Total", "Time (s)": 643.3497675290018, "type": "concurrent"}, {"store_sk": "529", "table name": "Total", "Time (s)": 315.2401069259977, "type": "sequential"}, {"store_sk": "650", "table name": "Total", "Time (s)": 207.37589281599867, "type": "sequential"}, {"store_sk": "14", "table name": "Total", "Time (s)": 256.8692958850006, "type": "sequential"}, {"store_sk": "406", "table name": "Total", "Time (s)": 464.80179426700124, "type": "sequential"}, {"store_sk": "178", "table name": "Total", "Time (s)": 772.7507812119984, "type": "sequential"}]}}, {"mode": "vega-lite"});
</script>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The above chart shows the time it would have taken had the last run executed sequentially. I have summed the timing for each table and plotted it together with the conurrent execution metrics.</p>
<p>Time diference between concurrent and sequential execution may not be huge but that is because the <code>store_returns</code> table has very less record as compared with <code>store_sales</code> to make a significant difference. However in practise this scales much better with more tables. In my project I had to insert data into 25 tables and using this similar concurrency I was able to reduce the time by over 70%. An important thing to note here is degree of parallelization will depends upon your cluster capacity. With more tables you will need to increase the workers in the cluster. Experiment with different numbers to find sweet spot of best performance vs cost ratio for your use case.</p>
<p>If you have any questions leave it a comment below. In next post we will discuss how bulk loading performs with different indexing strategy and benchmark them.</p>

</div>
</div>
</div>
</div>



  </div><a class="u-url" href="/blog/2020/08/bulk-insert-into-sql-using-mssql-spark-connector/" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/blog/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/blog/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/blog/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
        <ul class="contact-list">
          <li class="p-name">Ankit Sinha</li>
          
        </ul>
      </div>
      <div class="footer-col">
        <p>A technology blog focusing on random stuff</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/ankitbko" title="ankitbko"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/ankitbko" title="ankitbko"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
