<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Building large scale data ingestion solutions for Azure SQL using Azure databricks - Part 1 | F5 - Squashing Bugs</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Building large scale data ingestion solutions for Azure SQL using Azure databricks - Part 1" />
<meta name="author" content="Ankit Sinha" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Discover how to bulk insert million of rows into Azure SQL Hyperscale using Databricks" />
<meta property="og:description" content="Discover how to bulk insert million of rows into Azure SQL Hyperscale using Databricks" />
<link rel="canonical" href="https://ankitbko.github.io/blog/2020/09/bulk-import-using-sql-spark-connector-p1/" />
<meta property="og:url" content="https://ankitbko.github.io/blog/2020/09/bulk-import-using-sql-spark-connector-p1/" />
<meta property="og:site_name" content="F5 - Squashing Bugs" />
<meta property="og:image" content="https://ankitbko.github.io/blog/images/previews/spark-connector-1-preview.svg" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-09-01T00:00:00-05:00" />
<script type="application/ld+json">
{"url":"https://ankitbko.github.io/blog/2020/09/bulk-import-using-sql-spark-connector-p1/","@type":"BlogPosting","headline":"Building large scale data ingestion solutions for Azure SQL using Azure databricks - Part 1","dateModified":"2020-09-01T00:00:00-05:00","datePublished":"2020-09-01T00:00:00-05:00","image":"https://ankitbko.github.io/blog/images/previews/spark-connector-1-preview.svg","mainEntityOfPage":{"@type":"WebPage","@id":"https://ankitbko.github.io/blog/2020/09/bulk-import-using-sql-spark-connector-p1/"},"author":{"@type":"Person","name":"Ankit Sinha"},"description":"Discover how to bulk insert million of rows into Azure SQL Hyperscale using Databricks","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/blog/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://ankitbko.github.io/blog/feed.xml" title="F5 - Squashing Bugs" /><!-- the google_analytics_id gets auto inserted from the config file -->



<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-75679348-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-75679348-1');
</script>


<link rel="shortcut icon" type="image/x-icon" href="/blog/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" integrity="sha512-h7nl+xz8wgDlNM4NqKEM4F1NkIRS17M9+uJwIGwuo8vGqIl4BhuCKdxjWEINm+xyrUjNCnK5dCrhM0sj+wTIXw==" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.js" integrity="sha512-/CMIhXiDA3m2c9kzRyd97MTb3MC6OVnx4TElQ7fkkoRghwDf6gi41gaT1PwF270W6+J60uTmwgeRpNpJdRV6sg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/contrib/auto-render.min.js" integrity="sha512-Do7uJAaHZm5OLrIv/yN4w0iG1dbu01kzdMNnFfu/mAqgUk6Nniv2JYHcwH+cNwjqgLcqcuBBk+JRvprLVI8azg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A==" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/blog/">F5 - Squashing Bugs</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/blog/about/">About Me</a><a class="page-link" href="/blog/search/">Search</a><a class="page-link" href="/blog/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Building large scale data ingestion solutions for Azure SQL using Azure databricks - Part 1</h1><p class="page-description">Discover how to bulk insert million of rows into Azure SQL Hyperscale using Databricks</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2020-09-01T00:00:00-05:00" itemprop="datePublished">
        Sep 1, 2020
      </time>
       â€¢ <span class="read-time" title="Estimated read time">
    
    
      16 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/blog/categories/#spark">spark</a>
        &nbsp;
      
        <a class="category-tags-link" href="/blog/categories/#Azure Databricks">Azure Databricks</a>
        &nbsp;
      
        <a class="category-tags-link" href="/blog/categories/#Azure SQL">Azure SQL</a>
        &nbsp;
      
        <a class="category-tags-link" href="/blog/categories/#data ingestion">data ingestion</a>
        &nbsp;
      
        <a class="category-tags-link" href="/blog/categories/#SQL spark connector">SQL spark connector</a>
        &nbsp;
      
        <a class="category-tags-link" href="/blog/categories/#big data">big data</a>
        &nbsp;
      
        <a class="category-tags-link" href="/blog/categories/#python">python</a>
        
      
      </p>
    

    
      
		<div class="d-flex flex-wrap flex-justify-start flex-items-center">
			<p class="page-description" style="margin-right: .5rem;">Source Code </p>
			<div class="page-description">
				<div class="px-2">
    <a href="https://github.com/ankitbko/sql-spark-connector-sample" role="button" target="_blank">
        <img class="notebook-badge-image" src="/blog/assets/badges/github.svg" alt="View On GitHub">
    </a>
</div>

			</div>
		</div>
	
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-09-01-bulk-import-using-sql-spark-connector-p1.ipynb
-->

<div class="container" id="notebook-container">
        
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>With rise of big data, polyglot persistence and availability of cheaper storage technology it is becoming increasingly common to keep data into cheaper long term storage such as ADLS and load them into OLTP or OLAP databases as needed. In this 3 part blog series we will check out newly released <a href="https://github.com/microsoft/sql-spark-connector">Apache Spark Connector for SQL Server and Azure SQL</a> (refered as Microsoft SQL Spark Connector) and use it to insert large amount of data into Azure SQL Hyperscale. We will also capture benchmarks and also discuss some common problems we faced and solutions to them.</p>
<p>In this first post we will see to use Microsoft SQL Spark Connector to bulk load data into Azure SQL from Azure Data Lake and how to optimize it even further. In the second part we will capture and compare benchmarks of bulk loading large dataset into different Azure SQL databases each having different indexing strategy. And in the final post we will discuss an issue with deadlocks (that we will face along this journey) and potential solultion. In each of the posts in this series I will mention what environment and dataset I have used and also share direct links to the scripts used.</p>
<p>So lets get started.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Getting-started-with-Microsoft-SQL-Spark-Connector">Getting started with Microsoft SQL Spark Connector<a class="anchor-link" href="#Getting-started-with-Microsoft-SQL-Spark-Connector"> </a></h2><p>Microsoft SQL Spark Connector is an evolution of now deprecated <a href="https://github.com/Azure/azure-sqldb-spark">Azure SQL Spark Connector</a>. It provides hosts of different features to easily integrate with SQL Server and Azure SQL from spark. At the time of writing this blog, the connector is in active development and a release package is not yet published to maven repository. So in order to get it you can either download precompiled <em>jar</em> file from the <a href="https://github.com/microsoft/sql-spark-connector/releases">releases</a> tab in repository, or build the master branch locally. Once you have the jar file install it into your databricks cluster.<br />
<div class="flash">
    <svg class="octicon octicon-info" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M8 1.5a6.5 6.5 0 100 13 6.5 6.5 0 000-13zM0 8a8 8 0 1116 0A8 8 0 010 8zm6.5-.25A.75.75 0 017.25 7h1a.75.75 0 01.75.75v2.75h.25a.75.75 0 010 1.5h-2a.75.75 0 010-1.5h.25v-2h-.25a.75.75 0 01-.75-.75zM8 6a1 1 0 100-2 1 1 0 000 2z"></path></svg>
    <strong>Note: </strong>As of writing this post, the connector does not support Spark 3.0.
</div></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Environment">Environment<a class="anchor-link" href="#Environment"> </a></h2><p>We will be using Azure Databricks with cluster configurations as following -</p>
<ul>
<li>Cluster Mode: Standard</li>
<li>Databricks Runtime Version: 6.6 (includes Apache Spark 2.4.5, Scala 2.11)</li>
<li>Workers: 2 </li>
<li>Worker Type: Standard_DS3_v2 (14.0 GB Memory, 4 Cores, 0.75 DBU)</li>
<li>Driver Type: Standard_DS3_v2 (14.0 GB Memory, 4 Cores, 0.75 DBU)</li>
</ul>
<p>Libraries installed in the cluster -</p>
<ul>
<li>Microsoft SQL Spark Connector (jar) - Built at <a href="https://github.com/microsoft/sql-spark-connector/tree/c787e8ff854eb356c31092c9815e3a43617d9b4f">c787e8f</a></li>
<li>codetiming (PyPI) - Used to capture metrics</li>
<li>altair - Used to display charts</li>
</ul>
<p>Azure SQL Server Hyperscale configured at 2vCore and 0 replicas. In this post we will be using a single database which has tables as per this <a href="https://github.com/ankitbko/sql-spark-connector-sample/blob/master/sql/create_table.sql">SQL DDL script</a>.</p>
<p>Azure Data Lake Gen 2 contains parquet files for the dataset we use which is then <a href="https://docs.databricks.com/data/data-sources/azure/azure-datalake-gen2.html#mount-an-azure-data-lake-storage-gen2-account-using-a-service-principal-and-oauth-20">mounted</a> on Databricks.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Dataset">Dataset<a class="anchor-link" href="#Dataset"> </a></h2><p>We will be using 1 TB TPC-DS dataset v2.13.0rc1 for this blog series. Due to licensing restriction TPC-DS tool is not included the repo. However the toolkit is free to download <a href="http://www.tpc.org/tpcds/default5.asp">here</a>. However I did include a subset of SQL DDL statements for creating tables <a href="https://github.com/ankitbko/sql-spark-connector-sample/blob/master/sql/create_table.sql">here</a> that we use in this blog.</p>
<p>In this post we will be focusing on only 3 tables - store, store_returns and store_sales. As the name suggests <em>store_sales</em> and <em>store_returns</em> contains items sold or returned for different stores. For more information about the dataset refer to the specification document available in the TPC-DS toolkit.</p>
<p>There are no foreign key in the tables so Surrogate Key of store (<code>s_store_sk</code>) will be used to query and group results from store_sales (<code>ss_store_sk</code>) and store_returns (<code>sr_store_sk</code>).</p>
<p>This dataset was already available in my Azure SQL database and is then exported as parquet files using <a href="https://github.com/ankitbko/sql-spark-connector-sample/blob/master/notebooks/00-export-parquet-sql.ipynb">export notebook</a>. In case you have the dataset as text files you can modify the export notebook accordingly. 
<div class="flash">
    <svg class="octicon octicon-info octicon octicon-info" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M8 1.5a6.5 6.5 0 100 13 6.5 6.5 0 000-13zM0 8a8 8 0 1116 0A8 8 0 010 8zm6.5-.25A.75.75 0 017.25 7h1a.75.75 0 01.75.75v2.75h.25a.75.75 0 010 1.5h-2a.75.75 0 010-1.5h.25v-2h-.25a.75.75 0 01-.75-.75zM8 6a1 1 0 100-2 1 1 0 000 2z"></path></svg>
    <strong>Note: </strong>If you generate parquet files from any text format such as (csv, pipe-separated etc) you will need to correctly specify schema while writing to parquet. You can find the schema in specification document of TPC-DS toolkit.
</div></p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<style scoped="">
  .ansiout {
    display: block;
    unicode-bidi: embed;
    white-space: pre-wrap;
    word-wrap: break-word;
    word-break: break-all;
    font-family: "Source Code Pro", "Menlo", monospace;;
    font-size: 13px;
    color: #555;
    margin-left: 4px;
    line-height: 19px;
  }
</style>
<div class="ansiout">Out[1]: [&#39;dbfs:/mnt/adls/adls7dataset7benchmark/tpcds1tb/parquet/store_sales/_SUCCESS&#39;,
 &#39;dbfs:/mnt/adls/adls7dataset7benchmark/tpcds1tb/parquet/store_sales/ss_store_sk=1/&#39;,
 &#39;dbfs:/mnt/adls/adls7dataset7benchmark/tpcds1tb/parquet/store_sales/ss_store_sk=10/&#39;,
 &#39;dbfs:/mnt/adls/adls7dataset7benchmark/tpcds1tb/parquet/store_sales/ss_store_sk=100/&#39;,
 &#39;dbfs:/mnt/adls/adls7dataset7benchmark/tpcds1tb/parquet/store_sales/ss_store_sk=1000/&#39;]</div>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Notice in the above output the <em>store_sales</em> dataset is partitioned on <code>ss_store_sk</code>. This partitioning strategy works for us as we will be filtering on <code>ss_store_sk</code> and only work with few of the stores. This will also set us up for an issue that we will face later. The same partitioning strategy is used for <em>store_returns</em> as well.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Bulk-Loading-data-into-Azure-SQL-Database">Bulk Loading data into Azure SQL Database<a class="anchor-link" href="#Bulk-Loading-data-into-Azure-SQL-Database"> </a></h2><p>Our use case will be to load sales and returns for a particular <em>store</em> into Azure SQL database having row store indexes (Primary Key) on table. This means we will have to load data for each store from <em>store</em> table and all its associated sales and returns from <em>store_sales</em> and <em>store_returns</em> tables respectively. Surrogate key for store is what ties together all three tables.</p>
<p>To compare results we will capture the time it takes to insert records into each table. Therefore there are some boilerplate code that captures metrics which can be safely removed. Anything within <code># --- Capturing Metrics Start---</code> and <code># --- Capturing Metrics End---</code> block is used solely for capturing metrics and can be safely ignored. It does not have any impact on importing data into Azure SQL.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">codetiming</span> <span class="kn">import</span> <span class="n">Timer</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="n">path</span> <span class="o">=</span> <span class="s2">&quot;/mnt/adls/adls7dataset7benchmark/tpcds1tb/parquet/&quot;</span>
<span class="n">server_name</span> <span class="o">=</span> <span class="n">dbutils</span><span class="o">.</span><span class="n">secrets</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">scope</span> <span class="o">=</span> <span class="s2">&quot;kvbenchmark&quot;</span><span class="p">,</span> <span class="n">key</span> <span class="o">=</span> <span class="s2">&quot;db-server-url&quot;</span><span class="p">)</span>
<span class="n">username</span> <span class="o">=</span> <span class="n">dbutils</span><span class="o">.</span><span class="n">secrets</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">scope</span> <span class="o">=</span> <span class="s2">&quot;kvbenchmark&quot;</span><span class="p">,</span> <span class="n">key</span> <span class="o">=</span> <span class="s2">&quot;db-username&quot;</span><span class="p">)</span>
<span class="n">password</span> <span class="o">=</span> <span class="n">dbutils</span><span class="o">.</span><span class="n">secrets</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">scope</span> <span class="o">=</span> <span class="s2">&quot;kvbenchmark&quot;</span><span class="p">,</span> <span class="n">key</span> <span class="o">=</span> <span class="s2">&quot;db-password&quot;</span><span class="p">)</span>
<span class="n">schema</span> <span class="o">=</span> <span class="s2">&quot;dbo&quot;</span>

<span class="n">database_name</span> <span class="o">=</span> <span class="s2">&quot;noidx&quot;</span>
<span class="n">url</span> <span class="o">=</span> <span class="n">server_name</span> <span class="o">+</span> <span class="s2">&quot;;&quot;</span> <span class="o">+</span> <span class="s2">&quot;databaseName=&quot;</span> <span class="o">+</span> <span class="n">database_name</span> <span class="o">+</span> <span class="s2">&quot;;&quot;</span>
<span class="n">url</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<style scoped="">
  .ansiout {
    display: block;
    unicode-bidi: embed;
    white-space: pre-wrap;
    word-wrap: break-word;
    word-break: break-all;
    font-family: "Source Code Pro", "Menlo", monospace;;
    font-size: 13px;
    color: #555;
    margin-left: 4px;
    line-height: 19px;
  }
</style>
<div class="ansiout">Out[2]: &#39;[REDACTED];databaseName=noidx;&#39;</div>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We start with importing relevant packages and getting secrets from our secret-store. In case you want to run the notebook for yourself, you will need to modify the above cell accordingly. I am using <code>codetiming</code> and <code>pandas</code> packages to capture metrics and is not needed for bulk importing itself.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">pyspark.sql.types</span> <span class="kn">import</span> <span class="o">*</span>

<span class="c1"># List of table names</span>
<span class="n">tables</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;store&quot;</span><span class="p">,</span> <span class="s2">&quot;store_returns&quot;</span><span class="p">,</span> <span class="s2">&quot;store_sales&quot;</span><span class="p">]</span>

<span class="c1"># Map between table names and store surrogate key</span>
<span class="n">table_storesk_map</span> <span class="o">=</span> <span class="p">{</span>
  <span class="s2">&quot;store&quot;</span><span class="p">:</span> <span class="s2">&quot;s_store_sk&quot;</span><span class="p">,</span> 
  <span class="s2">&quot;store_returns&quot;</span><span class="p">:</span> <span class="s2">&quot;sr_store_sk&quot;</span><span class="p">,</span>
  <span class="s2">&quot;store_sales&quot;</span><span class="p">:</span> <span class="s2">&quot;ss_store_sk&quot;</span>
<span class="p">}</span>

<span class="c1"># Map between table names and schema</span>
<span class="n">table_schema_map</span> <span class="o">=</span> <span class="p">{</span>
  <span class="s2">&quot;store&quot;</span><span class="p">:</span> <span class="p">[</span>
    <span class="n">StructField</span><span class="p">(</span><span class="s2">&quot;s_store_sk&quot;</span><span class="p">,</span> <span class="n">IntegerType</span><span class="p">(),</span> <span class="kc">False</span><span class="p">),</span>
    <span class="n">StructField</span><span class="p">(</span><span class="s2">&quot;s_store_id&quot;</span><span class="p">,</span> <span class="n">StringType</span><span class="p">(),</span> <span class="kc">False</span><span class="p">)</span>
  <span class="p">],</span>
  <span class="s2">&quot;store_returns&quot;</span><span class="p">:</span> <span class="p">[</span>
    <span class="n">StructField</span><span class="p">(</span><span class="s2">&quot;sr_item_sk&quot;</span><span class="p">,</span> <span class="n">IntegerType</span><span class="p">(),</span> <span class="kc">False</span><span class="p">),</span> 
    <span class="n">StructField</span><span class="p">(</span><span class="s2">&quot;sr_ticket_number&quot;</span><span class="p">,</span> <span class="n">IntegerType</span><span class="p">(),</span> <span class="kc">False</span><span class="p">),</span>
  <span class="p">],</span>
  <span class="s2">&quot;store_sales&quot;</span><span class="p">:</span> <span class="p">[</span>
    <span class="n">StructField</span><span class="p">(</span><span class="s2">&quot;ss_item_sk&quot;</span><span class="p">,</span> <span class="n">IntegerType</span><span class="p">(),</span> <span class="kc">False</span><span class="p">),</span> 
    <span class="n">StructField</span><span class="p">(</span><span class="s2">&quot;ss_ticket_number&quot;</span><span class="p">,</span> <span class="n">IntegerType</span><span class="p">(),</span> <span class="kc">False</span><span class="p">)</span>
  <span class="p">]</span>
<span class="p">}</span>

<span class="c1"># List of stores that we use</span>
<span class="n">stores</span> <span class="o">=</span> <span class="p">[</span><span class="n">row</span><span class="p">[</span><span class="s2">&quot;ss_store_sk&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">parquet</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">path</span><span class="si">}</span><span class="s2">/store_sales&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">filter</span><span class="p">(</span><span class="s2">&quot;ss_store_sk IS NOT null&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">groupBy</span><span class="p">(</span><span class="s1">&#39;ss_store_sk&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">count</span><span class="p">()</span><span class="o">.</span><span class="n">orderBy</span><span class="p">(</span><span class="s1">&#39;count&#39;</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="s2">&quot;ss_store_sk&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="mi">5</span><span class="p">)]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">stores</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<style scoped="">
  .ansiout {
    display: block;
    unicode-bidi: embed;
    white-space: pre-wrap;
    word-wrap: break-word;
    word-break: break-all;
    font-family: "Source Code Pro", "Menlo", monospace;;
    font-size: 13px;
    color: #555;
    margin-left: 4px;
    line-height: 19px;
  }
</style>
<div class="ansiout">[529, 650, 14, 406, 178]
</div>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Next we initialize the tables and <em>stores</em> that we will use for importing into Azure SQL. <code>table_schema_map</code> is used to correct the schema to match that of database. This is a temporary workaround for <a href="https://github.com/microsoft/sql-spark-connector/issues/5">issue #5</a>. We will be bulk inserting data in <em>store</em>, <em>store_sales</em> and <em>store_returns</em> for 5 stores having highest number of records in <em>store_sales</em>. The count of records for each of the stores are as below. Overall we will be inserting ~30 million records into our database.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>store</th>
      <th>store_returns</th>
      <th>store_sales</th>
      <th>Total</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>529</th>
      <td>1</td>
      <td>555343</td>
      <td>5512441</td>
      <td>6067785</td>
    </tr>
    <tr>
      <th>650</th>
      <td>1</td>
      <td>555966</td>
      <td>5510505</td>
      <td>6066472</td>
    </tr>
    <tr>
      <th>14</th>
      <td>1</td>
      <td>554904</td>
      <td>5508259</td>
      <td>6063164</td>
    </tr>
    <tr>
      <th>406</th>
      <td>1</td>
      <td>554828</td>
      <td>5506912</td>
      <td>6061741</td>
    </tr>
    <tr>
      <th>178</th>
      <td>1</td>
      <td>554147</td>
      <td>5506321</td>
      <td>6060469</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>I have a method to truncate the tables that we are working with. This will make it easier for us to rerun any of tests without getting into trouble with duplicate Primary key.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">truncate_tables</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">username</span><span class="p">,</span> <span class="n">password</span><span class="p">,</span> <span class="n">tables</span><span class="p">):</span>
  <span class="k">for</span> <span class="n">table</span> <span class="ow">in</span> <span class="n">tables</span><span class="p">:</span>
    <span class="n">query</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;TRUNCATE TABLE </span><span class="si">{</span><span class="n">schema</span><span class="si">}</span><span class="s2">.</span><span class="si">{</span><span class="n">table</span><span class="si">}</span><span class="s2">&quot;</span>

    <span class="k">try</span><span class="p">:</span>
      <span class="n">t</span> <span class="o">=</span> <span class="n">Timer</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;Tuncated table </span><span class="si">{</span><span class="n">table</span><span class="si">}</span><span class="s2"> in: </span><span class="se">{{</span><span class="s2">:0.2f</span><span class="se">}}</span><span class="s2">&quot;</span><span class="p">)</span>
      <span class="n">t</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>
      <span class="n">driver_manager</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">_sc</span><span class="o">.</span><span class="n">_gateway</span><span class="o">.</span><span class="n">jvm</span><span class="o">.</span><span class="n">java</span><span class="o">.</span><span class="n">sql</span><span class="o">.</span><span class="n">DriverManager</span>
      <span class="n">con</span> <span class="o">=</span> <span class="n">driver_manager</span><span class="o">.</span><span class="n">getConnection</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">username</span><span class="p">,</span> <span class="n">password</span><span class="p">)</span>

      <span class="n">stmt</span> <span class="o">=</span> <span class="n">con</span><span class="o">.</span><span class="n">createStatement</span><span class="p">()</span>
      <span class="n">stmt</span><span class="o">.</span><span class="n">executeUpdate</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
      <span class="n">stmt</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
      <span class="n">t</span><span class="o">.</span><span class="n">stop</span><span class="p">()</span>
    <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
      <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Failed to truncate table </span><span class="si">{</span><span class="n">table</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">e</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<style scoped="">
  .ansiout {
    display: block;
    unicode-bidi: embed;
    white-space: pre-wrap;
    word-wrap: break-word;
    word-break: break-all;
    font-family: "Source Code Pro", "Menlo", monospace;;
    font-size: 13px;
    color: #555;
    margin-left: 4px;
    line-height: 19px;
  }
</style>
<div class="ansiout"></div>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Inserting-into-each-table-sequentially">Inserting into each table sequentially<a class="anchor-link" href="#Inserting-into-each-table-sequentially"> </a></h3>
</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">create_valid_table_schema</span><span class="p">(</span><span class="n">table_name</span><span class="p">,</span> <span class="n">df_schema</span><span class="p">):</span> 
  <span class="k">return</span> <span class="n">StructType</span><span class="p">([</span><span class="n">x</span> <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="nb">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">n</span><span class="p">:</span> <span class="n">n</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">table_schema_map</span><span class="p">[</span><span class="n">table_name</span><span class="p">])</span> <span class="k">else</span> <span class="nb">next</span><span class="p">(</span><span class="nb">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">f</span><span class="p">:</span> <span class="n">f</span><span class="o">.</span><span class="n">name</span> <span class="o">==</span> <span class="n">x</span><span class="o">.</span><span class="n">name</span> <span class="p">,</span><span class="n">table_schema_map</span><span class="p">[</span><span class="n">table_name</span><span class="p">]))</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">df_schema</span><span class="p">])</span>

<span class="k">def</span> <span class="nf">import_single_store</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">table_name</span><span class="p">,</span> <span class="n">store_sk</span><span class="p">):</span> 
  <span class="k">try</span><span class="p">:</span>
      <span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">parquet</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">path</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">table_name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
      <span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">table_storesk_map</span><span class="p">[</span><span class="n">table_name</span><span class="p">]</span><span class="si">}</span><span class="s2"> == </span><span class="si">{</span><span class="n">store_sk</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

      <span class="c1"># Temporary workaround until Issue #5 gets fixed https://github.com/microsoft/sql-spark-connector/issues/5</span>
      <span class="n">table_schema</span> <span class="o">=</span> <span class="n">create_valid_table_schema</span><span class="p">(</span><span class="n">table_name</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">schema</span><span class="p">)</span>
      <span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">rdd</span><span class="p">,</span> <span class="n">table_schema</span><span class="p">)</span>

      <span class="n">t</span> <span class="o">=</span> <span class="n">Timer</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;Imported store </span><span class="si">{</span><span class="n">store_sk</span><span class="si">}</span><span class="s2"> into table </span><span class="si">{</span><span class="n">table_name</span><span class="si">}</span><span class="s2"> in : </span><span class="se">{{</span><span class="s2">:0.2f</span><span class="se">}}</span><span class="s2"> &quot;</span><span class="p">)</span>    
      <span class="n">t</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>
      
      <span class="n">df</span><span class="o">.</span><span class="n">write</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s2">&quot;com.microsoft.sqlserver.jdbc.spark&quot;</span><span class="p">)</span> \
        <span class="o">.</span><span class="n">mode</span><span class="p">(</span><span class="s2">&quot;append&quot;</span><span class="p">)</span> \
        <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&quot;url&quot;</span><span class="p">,</span> <span class="n">url</span><span class="p">)</span> \
        <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&quot;dbtable&quot;</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">schema</span><span class="si">}</span><span class="s2">.</span><span class="si">{</span><span class="n">table_name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span> \
        <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="n">username</span><span class="p">)</span> \
        <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&quot;password&quot;</span><span class="p">,</span> <span class="n">password</span><span class="p">)</span> \
        <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&quot;tableLock&quot;</span><span class="p">,</span> <span class="s2">&quot;false&quot;</span><span class="p">)</span> \
        <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&quot;batchsize&quot;</span><span class="p">,</span> <span class="s2">&quot;100000&quot;</span><span class="p">)</span> \
        <span class="o">.</span><span class="n">save</span><span class="p">()</span>

      <span class="n">elapsed</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">stop</span><span class="p">()</span>
      <span class="k">return</span> <span class="n">elapsed</span>
  <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Failed to import </span><span class="si">{</span><span class="n">store_sk</span><span class="si">}</span><span class="s2"> into table </span><span class="si">{</span><span class="n">table_name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">e</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<style scoped="">
  .ansiout {
    display: block;
    unicode-bidi: embed;
    white-space: pre-wrap;
    word-wrap: break-word;
    word-break: break-all;
    font-family: "Source Code Pro", "Menlo", monospace;;
    font-size: 13px;
    color: #555;
    margin-left: 4px;
    line-height: 19px;
  }
</style>
<div class="ansiout"></div>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>These two methods will be reused throught this blog. <code>create_valid_table_schema</code> method takes the schema of the parquet files and merges it with schema defined in <code>table_schema_map</code>. This is a temporary fix as discussed before.</p>
<p><code>import_single_store(url, table_name, store_sk)</code> takes in the url of the database, the table name and the surrogate key (<code>store_sk</code>) of the store. It then reads the parquet file for the specified table filtered by <code>store_sk</code>. It corrects the schema, starts the timer and submits the insertion job to spark. The two parameters <code>tableLock</code> and <code>batchsize</code> are crucial.</p>
<ul>
<li><code>tableLock</code> or <a href="https://docs.microsoft.com/en-us/sql/t-sql/queries/hints-transact-sql-table?view=sql-server-ver15">TABLOCK</a> specifies that the acquired lock is applied at the table level. This option is helpful when inserting into Heap table.</li>
<li><code>batchsize</code> describes how many rows are committed at a time during the bulk operation. Play around with it to see how the system performs with different batch sizes. This option will become useful later when we bulk insert into a Clustered Columnstore Index (CCI).</li>
</ul>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">import_stores</span><span class="p">(</span><span class="n">url</span><span class="p">):</span>
  <span class="k">for</span> <span class="n">store_sk</span> <span class="ow">in</span> <span class="n">stores</span><span class="p">:</span>      
      <span class="c1"># -------- Capturing Metrics Start---------</span>
      <span class="n">t</span> <span class="o">=</span> <span class="n">Timer</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;Imported store </span><span class="si">{</span><span class="n">store_sk</span><span class="si">}</span><span class="s2"> in : </span><span class="se">{{</span><span class="s2">:0.2f</span><span class="se">}}</span><span class="s2">&quot;</span><span class="p">)</span>
      <span class="n">t</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>
      <span class="n">metrics_row</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;store_sk&#39;</span><span class="p">:</span> <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">store_sk</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">}</span>
      <span class="c1"># -------- Capturing Metrics End---------</span>
      
      <span class="k">for</span> <span class="n">table_name</span> <span class="ow">in</span> <span class="n">table_storesk_map</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
        <span class="n">elapsed</span> <span class="o">=</span> <span class="n">import_single_store</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">table_name</span><span class="p">,</span> <span class="n">store_sk</span><span class="p">)</span>

        <span class="c1"># -------- Capturing Metrics Start---------</span>
        <span class="n">metrics_row</span><span class="p">[</span><span class="n">table_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">elapsed</span>
      
      <span class="n">elapsed</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">stop</span><span class="p">()</span>
      <span class="n">metrics_row</span><span class="p">[</span><span class="s1">&#39;Total&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">elapsed</span>
      <span class="k">global</span> <span class="n">metrics_seq_df</span>
      <span class="n">metrics_seq_df</span> <span class="o">=</span> <span class="n">metrics_seq_df</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">metrics_row</span><span class="p">,</span> <span class="n">ignore_index</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
      <span class="c1"># -------- Capturing Metrics End---------</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<style scoped="">
  .ansiout {
    display: block;
    unicode-bidi: embed;
    white-space: pre-wrap;
    word-wrap: break-word;
    word-break: break-all;
    font-family: "Source Code Pro", "Menlo", monospace;;
    font-size: 13px;
    color: #555;
    margin-left: 4px;
    line-height: 19px;
  }
</style>
<div class="ansiout"></div>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><code>import_stores</code> loops over all stores and calls <code>import_single_store</code> for each table. Note that it waits for <code>import_single_store</code> to complete before continuing the loop and inserting into next table. Lets truncate the tables and run it to capture the reults.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">truncate_tables</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">username</span><span class="p">,</span> <span class="n">password</span><span class="p">,</span> <span class="n">tables</span><span class="p">)</span>
<span class="n">import_stores</span><span class="p">(</span><span class="n">url</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<style scoped="">
  .ansiout {
    display: block;
    unicode-bidi: embed;
    white-space: pre-wrap;
    word-wrap: break-word;
    word-break: break-all;
    font-family: "Source Code Pro", "Menlo", monospace;;
    font-size: 13px;
    color: #555;
    margin-left: 4px;
    line-height: 19px;
  }
</style>
<div class="ansiout">Tuncated table store in: 0.17
Tuncated table store_returns in: 0.06
Tuncated table store_sales in: 0.03
Imported store 529 into table store in : 2.72 
Imported store 529 into table store_returns in : 27.67 
Imported store 529 into table store_sales in : 276.37 
Imported store 529 in : 311.85
Imported store 650 into table store in : 0.59 
Imported store 650 into table store_returns in : 26.83 
Imported store 650 into table store_sales in : 151.32 
Imported store 650 in : 184.33
Imported store 14 into table store in : 0.31 
Imported store 14 into table store_returns in : 28.14 
Imported store 14 into table store_sales in : 182.92 
Imported store 14 in : 216.08
Imported store 406 into table store in : 0.43 
Imported store 406 into table store_returns in : 42.02 
Imported store 406 into table store_sales in : 310.86 
Imported store 406 in : 358.28
Imported store 178 into table store in : 0.31 
Imported store 178 into table store_returns in : 104.81 
Imported store 178 into table store_sales in : 581.17 
Imported store 178 in : 691.08
</div>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The import is going to succeed and the timings will be logged for each table.</p>
<p>There are two interesting things to notice when the jobs are running. First is that only one spark job runs at a time. This is because we are inserting into one table at a time and the spark will create only 1 job for each <code>spark.write</code> when inserting data into Azure SQL. This is important fact to keep in mind to optimize in future.</p>
<p><img src="/blog/images/copied_from_nb/./assets/images/posts/spark-connector-1/spark_jobs.png" alt="spark_job" /></p>
<p>The second is a little tricky to find. If you check the stages of running job when it inserts into <code>store_sales</code> table in Spark UI you will notice some tasks will fail due to Deadlock. 
<code>com.microsoft.sqlserver.jdbc.SQLServerException: Transaction (Process ID 99) was deadlocked on lock resources with another process and has been chosen as the deadlock victim. Rerun the transaction.</code></p>
<p>Although these tasks failed the stage and job overall succeeded and this is due to retry mechanism within Databricks. This can be confirmed by counting number of records in database to perform a quick sanity check. We let it be for now as we will be discussing the root cause of deadlock and potential solutions in part 3 of this blog.</p>
<p><img src="/blog/images/copied_from_nb/./assets/images/posts/spark-connector-1/deadlock.png" alt="deadlock" /></p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>store_sk</th>
      <th>store</th>
      <th>store_returns</th>
      <th>store_sales</th>
      <th>Total</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>529</td>
      <td>2.717032</td>
      <td>27.669652</td>
      <td>276.367528</td>
      <td>311.850131</td>
    </tr>
    <tr>
      <th>1</th>
      <td>650</td>
      <td>0.592542</td>
      <td>26.830166</td>
      <td>151.320050</td>
      <td>184.332525</td>
    </tr>
    <tr>
      <th>2</th>
      <td>14</td>
      <td>0.307472</td>
      <td>28.139986</td>
      <td>182.920102</td>
      <td>216.076222</td>
    </tr>
    <tr>
      <th>3</th>
      <td>406</td>
      <td>0.427487</td>
      <td>42.020107</td>
      <td>310.856628</td>
      <td>358.281521</td>
    </tr>
    <tr>
      <th>4</th>
      <td>178</td>
      <td>0.307313</td>
      <td>104.811686</td>
      <td>581.166653</td>
      <td>691.082328</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>If you look at the timings it becomes obvious that overall time it took to insert all the data related to single store is sum of time it took to insert into each of the tables for that store. But can we do any better? Ususally in most system partially inserted data is not of use so we are delayed till the data is inserted for all related tables. So our next objective is to reduce the time it takes to insert all data for a single store. Wouldn't it be great if we can insert data for single store in all the tables concurrently?</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Concurrently-inserting-data-in-all-tables">Concurrently inserting data in all tables<a class="anchor-link" href="#Concurrently-inserting-data-in-all-tables"> </a></h3><p>To achieve this we will utilize Python's multithreading library to submit multiple spark jobs concurrently. As you would know, anything we write and execute through databricks notebook runs on executor. And when executor find statement related to spark, it submits the job to spark and spark then orchestrates its execution on the workers. The observation from above tells us that only single job (and single stage) is executing in our cluster at any moment. This means our cluster is woefully underutilized. Our plan is to submit multiple spark jobs to better utilize our cluster.</p>
<p>To make it work properly with python we have to set <code>PYSPARK_PIN_THREAD</code> environment variable in python and set it to <code>true</code>. You need to do this as part of cluster startup as setting it though Python's <code>os</code> module does not work. You can find more details about it <a href="https://spark.apache.org/docs/2.4.5/job-scheduling.html#concurrent-jobs-in-pyspark">in the doc</a>. Note that as of yet this flag is not recommended in production. In case you want to perform such parallelization its better to use Scala. You can achieve same result in Scala using <code>Futures</code> and <code>Awaits</code>.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">threading</span>

<span class="k">class</span> <span class="nc">myThread</span> <span class="p">(</span><span class="n">threading</span><span class="o">.</span><span class="n">Thread</span><span class="p">):</span>
   <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">table_name</span><span class="p">,</span> <span class="n">store_sk</span><span class="p">):</span>
      <span class="n">threading</span><span class="o">.</span><span class="n">Thread</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">table_name</span> <span class="o">=</span> <span class="n">table_name</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">store_sk</span> <span class="o">=</span> <span class="n">store_sk</span>
  
   <span class="k">def</span> <span class="nf">run</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>      
      <span class="n">elapsed</span> <span class="o">=</span> <span class="n">import_single_store</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">table_name</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">store_sk</span><span class="p">)</span>

      <span class="c1"># -------- Capturing Metrics Start---------</span>
      <span class="k">global</span> <span class="n">metrics_con_df</span>
      <span class="n">metrics_con_df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">metrics_con_df</span><span class="p">[</span><span class="s1">&#39;store_sk&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">store_sk</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">table_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">elapsed</span>
      <span class="c1"># -------- Capturing Metrics End---------</span>


<span class="k">def</span> <span class="nf">import_stores_concurrent</span><span class="p">():</span>
  <span class="k">for</span> <span class="n">store_sk</span> <span class="ow">in</span> <span class="n">stores</span><span class="p">:</span>
      
      <span class="c1"># -------- Capturing Metrics Start---------</span>
      <span class="n">time</span> <span class="o">=</span> <span class="n">Timer</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;Imported store </span><span class="si">{</span><span class="n">store_sk</span><span class="si">}</span><span class="s2"> in : </span><span class="se">{{</span><span class="s2">:0.2f</span><span class="se">}}</span><span class="s2">&quot;</span><span class="p">)</span>
      <span class="n">time</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>
      
      <span class="k">global</span> <span class="n">metrics_con_df</span>
      <span class="n">metrics_con_df</span> <span class="o">=</span> <span class="n">metrics_con_df</span><span class="o">.</span><span class="n">append</span><span class="p">({</span><span class="s1">&#39;store_sk&#39;</span><span class="p">:</span> <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">store_sk</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">},</span> <span class="n">ignore_index</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
      <span class="c1"># -------- Capturing Metrics End---------</span>
            
      <span class="n">threads</span> <span class="o">=</span> <span class="p">[]</span>
      
      <span class="k">for</span> <span class="n">table_name</span> <span class="ow">in</span> <span class="n">tables</span><span class="p">:</span>
          <span class="n">thread</span> <span class="o">=</span> <span class="n">myThread</span><span class="p">(</span><span class="n">table_name</span><span class="p">,</span> <span class="n">store_sk</span><span class="p">)</span>
          <span class="n">thread</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>
          <span class="n">threads</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">thread</span><span class="p">)</span>

      <span class="c1"># Wait for all threads to complete</span>
      <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">threads</span><span class="p">:</span>
          <span class="n">t</span><span class="o">.</span><span class="n">join</span><span class="p">()</span>
      
      <span class="c1"># -------- Capturing Metrics Start--------- </span>
      <span class="n">elapsed</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">stop</span><span class="p">()</span>
      <span class="n">metrics_con_df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">metrics_con_df</span><span class="p">[</span><span class="s1">&#39;store_sk&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">store_sk</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="s1">&#39;Total&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">elapsed</span>
      <span class="c1"># -------- Capturing Metrics End---------</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<style scoped="">
  .ansiout {
    display: block;
    unicode-bidi: embed;
    white-space: pre-wrap;
    word-wrap: break-word;
    word-break: break-all;
    font-family: "Source Code Pro", "Menlo", monospace;;
    font-size: 13px;
    color: #555;
    margin-left: 4px;
    line-height: 19px;
  }
</style>
<div class="ansiout"></div>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We start with creating a class with most creative name ever - <code>myThread</code>. Each of myThread will execute <code>import_single_store</code> for a single table for a store. <code>import_stores_concurrent</code> loops through the stores and for each table it creates a new <code>myThread</code> that executes <code>import_single_store</code>. Each of this thread will submit a job to spark and spark will then handle the orchestration of the jobs on the cluster. After submitting all the jobs we wait for all the threads to finish before continuing with next store in loop. This means that for every store we will be inserting data from all three tables concurrently.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">truncate_tables</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">username</span><span class="p">,</span> <span class="n">password</span><span class="p">,</span> <span class="n">tables</span><span class="p">)</span>
<span class="n">import_stores_concurrent</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<style scoped="">
  .ansiout {
    display: block;
    unicode-bidi: embed;
    white-space: pre-wrap;
    word-wrap: break-word;
    word-break: break-all;
    font-family: "Source Code Pro", "Menlo", monospace;;
    font-size: 13px;
    color: #555;
    margin-left: 4px;
    line-height: 19px;
  }
</style>
<div class="ansiout">Tuncated table store in: 0.08
Tuncated table store_returns in: 0.08
Tuncated table store_sales in: 0.06
Imported store 529 into table store in : 0.29 
Imported store 529 into table store_returns in : 26.18 
Imported store 529 into table store_sales in : 274.94 
Imported store 529 in : 279.10
Imported store 650 into table store in : 3.33 
Imported store 650 into table store_returns in : 42.55 
Imported store 650 into table store_sales in : 161.23 
Imported store 650 in : 165.20
Imported store 14 into table store in : 0.27 
Imported store 14 into table store_returns in : 41.86 
Imported store 14 into table store_sales in : 213.83 
Imported store 14 in : 217.59
Imported store 406 into table store in : 0.30 
Imported store 406 into table store_returns in : 108.57 
Imported store 406 into table store_sales in : 331.98 
Imported store 406 in : 335.90
Imported store 178 into table store in : 0.32 
Imported store 178 into table store_returns in : 135.44 
Imported store 178 into table store_sales in : 704.72 
Imported store 178 in : 708.38
</div>
</div>

</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>store_sk</th>
      <th>store</th>
      <th>store_returns</th>
      <th>store_sales</th>
      <th>Total</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>529</td>
      <td>0.288181</td>
      <td>26.176</td>
      <td>274.942</td>
      <td>279.098</td>
    </tr>
    <tr>
      <th>1</th>
      <td>650</td>
      <td>3.32614</td>
      <td>42.5514</td>
      <td>161.225</td>
      <td>165.2</td>
    </tr>
    <tr>
      <th>2</th>
      <td>14</td>
      <td>0.272375</td>
      <td>41.8554</td>
      <td>213.833</td>
      <td>217.59</td>
    </tr>
    <tr>
      <th>3</th>
      <td>406</td>
      <td>0.304694</td>
      <td>108.566</td>
      <td>331.975</td>
      <td>335.902</td>
    </tr>
    <tr>
      <th>4</th>
      <td>178</td>
      <td>0.319758</td>
      <td>135.438</td>
      <td>704.716</td>
      <td>708.383</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This time you will notice that multiple spark jobs are running at same time. This becomes more evident once we look at the captured timings. The total time take for a single store is not the sum of each table but rather MAX for all 3 tables which in our case is <code>store_sales</code>. Now there may be some additional cost involved while scheduling jobs therefore the timings does not match exactly with the <code>store_sales</code>. However point to note here is that the time taken to insert into <code>store_returns</code> is completely absorbed by the timing of <code>store_sales</code>.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">

<div id="altair-viz-905eb7ee374f4b38975cc3b72af18c47"></div>
<script type="text/javascript">
  (function(spec, embedOpt){
    let outputDiv = document.currentScript.previousElementSibling;
    if (outputDiv.id !== "altair-viz-905eb7ee374f4b38975cc3b72af18c47") {
      outputDiv = document.getElementById("altair-viz-905eb7ee374f4b38975cc3b72af18c47");
    }
    const paths = {
      "vega": "https://cdn.jsdelivr.net/npm//vega@5?noext",
      "vega-lib": "https://cdn.jsdelivr.net/npm//vega-lib?noext",
      "vega-lite": "https://cdn.jsdelivr.net/npm//vega-lite@4.8.1?noext",
      "vega-embed": "https://cdn.jsdelivr.net/npm//vega-embed@6?noext",
    };

    function loadScript(lib) {
      return new Promise(function(resolve, reject) {
        var s = document.createElement('script');
        s.src = paths[lib];
        s.async = true;
        s.onload = () => resolve(paths[lib]);
        s.onerror = () => reject(`Error loading script: ${paths[lib]}`);
        document.getElementsByTagName("head")[0].appendChild(s);
      });
    }

    function showError(err) {
      outputDiv.innerHTML = `<div class="error" style="color:red;">${err}</div>`;
      throw err;
    }

    function displayChart(vegaEmbed) {
      vegaEmbed(outputDiv, spec, embedOpt)
        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));
    }

    if(typeof define === "function" && define.amd) {
      requirejs.config({paths});
      require(["vega-embed"], displayChart, err => showError(`Error loading script: ${err.message}`));
    } else if (typeof vegaEmbed === "function") {
      displayChart(vegaEmbed);
    } else {
      loadScript("vega")
        .then(() => loadScript("vega-lite"))
        .then(() => loadScript("vega-embed"))
        .catch(showError)
        .then(() => displayChart(vegaEmbed));
    }
  })({"config": {"view": {"continuousWidth": 400, "continuousHeight": 300}}, "data": {"name": "data-fac6c0a2c69463a15a1ef8946bbbe960"}, "mark": "bar", "encoding": {"color": {"type": "nominal", "field": "type"}, "column": {"type": "nominal", "field": "store_sk"}, "tooltip": [{"type": "nominal", "field": "store_sk"}, {"type": "nominal", "field": "type"}, {"type": "quantitative", "field": "Time (s)"}], "x": {"type": "nominal", "field": "type", "sort": {"field": "type", "order": "descending"}}, "y": {"type": "quantitative", "field": "Time (s)"}}, "$schema": "https://vega.github.io/schema/vega-lite/v4.8.1.json", "datasets": {"data-fac6c0a2c69463a15a1ef8946bbbe960": [{"store_sk": "529", "table name": "Total", "Time (s)": 279.09817834700016, "type": "concurrent"}, {"store_sk": "650", "table name": "Total", "Time (s)": 165.20011516800014, "type": "concurrent"}, {"store_sk": "14", "table name": "Total", "Time (s)": 217.59010785199962, "type": "concurrent"}, {"store_sk": "406", "table name": "Total", "Time (s)": 335.90167387600013, "type": "concurrent"}, {"store_sk": "178", "table name": "Total", "Time (s)": 708.3831757899998, "type": "concurrent"}, {"store_sk": "529", "table name": "Total", "Time (s)": 301.4059971890006, "type": "sequential"}, {"store_sk": "650", "table name": "Total", "Time (s)": 207.10258764499986, "type": "sequential"}, {"store_sk": "14", "table name": "Total", "Time (s)": 255.96043815599978, "type": "sequential"}, {"store_sk": "406", "table name": "Total", "Time (s)": 440.84587619700005, "type": "sequential"}, {"store_sk": "178", "table name": "Total", "Time (s)": 840.4732947220004, "type": "sequential"}]}}, {"mode": "vega-lite"});
</script>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The above chart shows how concurrent execution fairs against sequential execution. This is done by adding the timing for each table for the last run to emulate the result had the last execution ran sequentially.</p>
<p>Time diference between concurrent and sequential execution may not look big but that is because the <code>store_returns</code> table has very less record as compared with <code>store_sales</code> to make any significant difference. However in practise this scales much better with more number of tables. In my project I had to insert data into 26 tables and using the similar concurrency appraoch I was able to reduce the time by over 70%. An important point to note here is degree of parallelization will depends upon the cluster capacity. With more tables you will need more number of workers in the cluster. Experiment with different numbers to find sweet spot of best performance vs cost ratio for your use case.</p>
<p>If you have any questions leave it a comment below. In next post we will discuss how bulk loading performs against different indexing strategy and benchmark them.</p>

</div>
</div>
</div>
</div>



  </div>
  <div class="PageNavigation">
  
  <div class="prevDiv">
    <a class="prev" href="/blog/2020/08/device-code-flow-using-msal-in-python/">&laquo; Device Code Flow in Azure AD using Python's requests module and MSAL</a>
  </div>
  
  
  <div class="nextDiv">
    <a class="next" href="/blog/2020/09/bulk-import-using-sql-spark-connector-p2/">Building large scale data ingestion solutions for Azure SQL using Azure databricks - Part 2 &raquo;</a>
  </div>
  
</div>
<!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="ankitbko/blog"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/blog/2020/09/bulk-import-using-sql-spark-connector-p1/" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/blog/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/blog/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/blog/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
        <ul class="contact-list">
          <li class="p-name">Ankit Sinha</li>
          
        </ul>
      </div>
      <div class="footer-col">
        <p>A technology blog focusing on random stuff</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/ankitbko" title="ankitbko"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/ankitbko" title="ankitbko"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
